{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "357544f5",
   "metadata": {},
   "source": [
    "# Configura√ß√£o do Ambiente XAdapt-Drift\n",
    "\n",
    "Este notebook demonstra como usar a biblioteca XAdapt-Drift para an√°lise de drift em modelos de Machine Learning.\n",
    "\n",
    "## Configura√ß√£o do PYTHONPATH\n",
    "\n",
    "Primeiro, vamos adicionar o diret√≥rio raiz da biblioteca ao PYTHONPATH para permitir imports diretos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fbf1f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diret√≥rio do projeto: /home/alexandre/Documents/XDrift-Analyzer\n",
      "‚úÖ Diret√≥rio do projeto j√° est√° no PYTHONPATH\n",
      "‚úÖ XAdapt-Drift importado com sucesso!\n",
      "Localiza√ß√£o da biblioteca: /home/alexandre/Documents/XDrift-Analyzer/xadapt_drift/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# Configurar PYTHONPATH para importar a biblioteca XAdapt-Drift\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Obter o diret√≥rio raiz do projeto (um n√≠vel acima do diret√≥rio 'examples')\n",
    "project_root = Path.cwd().parent\n",
    "print(f\"Diret√≥rio do projeto: {project_root}\")\n",
    "\n",
    "# Adicionar ao PYTHONPATH se ainda n√£o estiver\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"‚úÖ Adicionado {project_root} ao PYTHONPATH\")\n",
    "else:\n",
    "    print(\"‚úÖ Diret√≥rio do projeto j√° est√° no PYTHONPATH\")\n",
    "\n",
    "# Verificar se a biblioteca pode ser importada\n",
    "try:\n",
    "    import xadapt_drift\n",
    "    print(\"‚úÖ XAdapt-Drift importado com sucesso!\")\n",
    "    print(f\"Localiza√ß√£o da biblioteca: {xadapt_drift.__file__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erro ao importar XAdapt-Drift: {e}\")\n",
    "    print(\"Verifique se voc√™ est√° executando o notebook do diret√≥rio correto.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f893d59",
   "metadata": {},
   "source": [
    "## üöÄ Exemplo B√°sico de Uso\n",
    "\n",
    "Agora que a biblioteca est√° configurada, vamos demonstrar um exemplo b√°sico de uso com o padr√£o adapter que discutimos anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc72af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo pr√°tico: Usando o SklearnAdapter com a BaseAdapter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importar classes da nossa biblioteca\n",
    "from xadapt_drift.adapters.sklearn_adapter import SklearnAdapter\n",
    "from xadapt_drift.drift.detector import DriftDetector\n",
    "\n",
    "print(\"üéØ Demonstra√ß√£o do Valor da BaseAdapter\\n\")\n",
    "\n",
    "# 1. Criar dados de exemplo\n",
    "print(\"üìä Criando dados de exemplo...\")\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "feature_names = [f\"feature_{i}\" for i in range(10)]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 2. Treinar modelo sklearn\n",
    "print(\"ü§ñ Treinando modelo RandomForest...\")\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Criar adapter usando nossa BaseAdapter\n",
    "print(\"üîå Criando SklearnAdapter...\")\n",
    "adapter = SklearnAdapter(\n",
    "    model=model,\n",
    "    feature_names=feature_names,\n",
    "    validate_model=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Adapter criado: {adapter}\")\n",
    "print(f\"üìã Informa√ß√µes do modelo: {adapter.get_model_info()['model_type']}\")\n",
    "print(f\"üìä Features: {len(adapter.feature_names)} features\")\n",
    "\n",
    "# 4. Usar o adapter (mesma interface independente do framework!)\n",
    "print(\"\\nüîÆ Fazendo predi√ß√µes...\")\n",
    "predictions = adapter.predict(X_test)\n",
    "print(f\"‚úÖ Predi√ß√µes realizadas: {predictions.shape}\")\n",
    "\n",
    "# 5. Gerar explica√ß√µes\n",
    "print(\"\\nüß† Gerando explica√ß√µes SHAP...\")\n",
    "try:\n",
    "    explanations = adapter.explain(X_test[:10], method=\"shap\")  # Usar apenas 10 amostras\n",
    "    print(f\"‚úÖ Explica√ß√µes geradas para {len(explanations)} features\")\n",
    "    \n",
    "    # Mostrar top 3 features mais importantes\n",
    "    sorted_features = sorted(explanations.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"üèÜ Top 3 features mais importantes:\")\n",
    "    for i, (feature, importance) in enumerate(sorted_features[:3], 1):\n",
    "        print(f\"   {i}. {feature}: {importance:.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è SHAP n√£o dispon√≠vel ou erro: {e}\")\n",
    "    print(\"üí° Tentando m√©todo de permutation importance...\")\n",
    "    \n",
    "    try:\n",
    "        explanations = adapter.explain(X_test[:10], y_test[:10], method=\"permutation\")\n",
    "        print(f\"‚úÖ Permutation importance gerada para {len(explanations)} features\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Erro tamb√©m com permutation: {e2}\")\n",
    "\n",
    "print(\"\\nüéâ Exemplo conclu√≠do! A BaseAdapter forneceu:\")\n",
    "print(\"   ‚úì Interface consistente independente do framework\")\n",
    "print(\"   ‚úì Valida√ß√£o autom√°tica de entrada\")\n",
    "print(\"   ‚úì Logging integrado\")\n",
    "print(\"   ‚úì Tratamento de erros robusto\")\n",
    "print(\"   ‚úì Metadados do modelo padronizados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b41d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import sys\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-v0_8-pastel')\n",
    "sns.set_palette('pastel')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Import XAdapt-Drift components\n",
    "from xadapt_drift import XAdaptDrift\n",
    "from xadapt_drift.adapters.sklearn_adapter import SklearnAdapter\n",
    "from xadapt_drift.utils.advanced_metrics import AdvancedDriftDetector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1c4e1",
   "metadata": {},
   "source": [
    "### Cria√ß√£o de dados - gera√ß√£o de dados sint√©ticos e fun√ß√µes de indu√ß√£o de Drift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c68eb580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_data(n_samples=10000, n_cat_features=3, n_num_features=5, seed=42):\n",
    "    \"\"\"Create a synthetic dataset with mixed data types\"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Create numerical features\n",
    "    X_numerical = np.random.randn(n_samples, n_num_features)\n",
    "    \n",
    "    # Create categorical features (3 categories each)\n",
    "    X_categorical = np.random.randint(0, 3, size=(n_samples, n_cat_features))\n",
    "\n",
    "    # Create target based on both numerical and categorical features\n",
    "    y = (0.5 * np.sum(X_numerical[:, :2], axis=1) + \n",
    "         0.8 * (X_categorical[:, 0] == 2).astype(int) - \n",
    "         0.5 * (X_categorical[:, 1] == 0).astype(int) + \n",
    "         0.1 * np.random.randn(n_samples)) > 0\n",
    "    \n",
    "    # Combine features\n",
    "    X = np.hstack([X_numerical, X_categorical])\n",
    "\n",
    "    # Create feature names\n",
    "    numerical_cols = [f'num_{i}' for i in range(n_num_features)]\n",
    "    categorical_cols = [f'cat_{i}' for i in range(n_cat_features)]\n",
    "    feature_names = numerical_cols + categorical_cols\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    \n",
    "    # Convert categorical columns to correct type\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "    return df, y.astype(int), numerical_cols, categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d866545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def induce_drift(df, num_cols, cat_cols, drift_type='mean_shift'):\n",
    "    \"\"\"Induce different types of drift in the dataset.\n",
    "    Args:\n",
    "        df: Original DataFrame\n",
    "        num_cols: List of numerical feature names\n",
    "        cat_cols: List of categorical feature names\n",
    "        drift_type: Type of drift to induce ('mean_shift', 'variance_change', 'category_frequency', 'multiple')\n",
    "    Returns:\n",
    "        drifted_df: DataFrame with induced drift\n",
    "        drifted_features: List of features that were changed\n",
    "    \"\"\"\n",
    "    \n",
    "    drifted_df = df.copy()\n",
    "    \n",
    "    if drift_type == 'mean_shift':\n",
    "        # Shift the mean of the first numerical feature\n",
    "        feature = num_cols[0]\n",
    "        shift = 1.5 * drifted_df[feature].std()\n",
    "        drifted_df[feature] += shift\n",
    "        drifted_features = [feature]\n",
    "        \n",
    "    elif drift_type == 'variance_change':\n",
    "        # Increase the variance of the second numerical feature\n",
    "        feature = num_cols[1]\n",
    "        drifted_df[feature] = drifted_df[feature] * 2.0\n",
    "        drifted_features = [feature]\n",
    "    \n",
    "    elif drift_type == 'category_frequency':\n",
    "        # Change the distribution of a categorical feature\n",
    "        feature = cat_cols[0]\n",
    "        # Find the least common category\n",
    "        least_common = drifted_df[feature].value_counts().idxmin()\n",
    "        # Make it more common by replacing some values\n",
    "        mask = np.random.choice([True, False], size=len(drifted_df), p=[0.4, 0.6])\n",
    "        drifted_df.loc[mask, feature] = least_common\n",
    "        drifted_features = [feature]\n",
    "        \n",
    "    elif drift_type == 'multiple':\n",
    "        # Induce multiple drifts\n",
    "        # Shift mean of first numerical feature\n",
    "        drifted_df[num_cols[0]] += 1.2 * drifted_df[num_cols[0]].std()\n",
    "        # Increase variance of second numerical feature\n",
    "        drifted_df[num_cols[1]] = drifted_df[num_cols[1]] * 1.8\n",
    "        # Change categorical distribution\n",
    "        feature = cat_cols[0]\n",
    "        mask = np.random.choice([True, False], size=len(drifted_df), p=[0.3, 0.7])\n",
    "        drifted_df.loc[mask, feature] = drifted_df[feature].value_counts().idxmin()\n",
    "        drifted_features = [num_cols[0], num_cols[1], cat_cols[0]]\n",
    "    \n",
    "    return drifted_df, drifted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b668f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def induce_specific_drifts(test_df, feature_names):\n",
    "    \"\"\"Induce specific types of drift for testing different metrics.\"\"\"\n",
    "    \n",
    "    scenarios = {}\n",
    "    \n",
    "    # Scenario 1: Gradual mean shift (detectable by KL/JS divergence)\n",
    "    scenario_1 = test_df.copy()\n",
    "    numerical_features = [f for f in feature_names if f.startswith('feature_')]\n",
    "    target_feature = numerical_features[0]\n",
    "    \n",
    "    # Gradual shift that creates different distribution shapes\n",
    "    shift_values = np.linspace(0, 2, len(scenario_1))\n",
    "    scenario_1[target_feature] += shift_values * scenario_1[target_feature].std()\n",
    "    scenarios['gradual_mean_shift'] = scenario_1\n",
    "    \n",
    "    # Scenario 2: Distribution shape change (strong KL divergence signal)\n",
    "    scenario_2 = test_df.copy()\n",
    "    target_feature = numerical_features[1]\n",
    "    \n",
    "    # Transform from normal to exponential-like distribution\n",
    "    original_data = scenario_2[target_feature]\n",
    "    # Apply exponential transformation while preserving some original characteristics\n",
    "    transformed_data = np.random.exponential(scale=np.abs(original_data.mean()), size=len(original_data))\n",
    "    scenario_2[target_feature] = transformed_data\n",
    "    scenarios['distribution_shape_change'] = scenario_2\n",
    "    \n",
    "    # Scenario 3: Categorical frequency drift (detectable by Chi-square and categorical KL)\n",
    "    scenario_3 = test_df.copy()\n",
    "    \n",
    "    # Change category distribution significantly\n",
    "    new_categories = np.random.choice(['Type_A', 'Type_B', 'Type_C'], \n",
    "                                    size=len(scenario_3), \n",
    "                                    p=[0.1, 0.2, 0.7])  # Very different from original [0.5, 0.3, 0.2]\n",
    "    scenario_3['category_1'] = new_categories\n",
    "    scenarios['categorical_frequency_drift'] = scenario_3\n",
    "    \n",
    "    # Scenario 4: Multiple subtle drifts (low individual signals, but cumulative effect)\n",
    "    scenario_4 = test_df.copy()\n",
    "    \n",
    "    # Small shifts in multiple features\n",
    "    for i, feature in enumerate(numerical_features[:4]):\n",
    "        shift = 0.3 * scenario_4[feature].std() * (i + 1) / 4  # Increasing shifts\n",
    "        scenario_4[feature] += shift\n",
    "    \n",
    "    # Slight categorical change\n",
    "    mask = np.random.choice([True, False], size=len(scenario_4), p=[0.2, 0.8])\n",
    "    scenario_4.loc[mask, 'category_2'] = 'Type_A'\n",
    "    scenarios['multiple_subtle_drifts'] = scenario_4\n",
    "    \n",
    "    return scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4644742",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df, y_ref, numerical_cols, categorical_cols = create_synthetic_data(n_samples=10000)\n",
    "print(f\"Created dataset with {len(numerical_cols)} numerical features and {len(categorical_cols)} categorical features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dbe8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c2065",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863bf3d",
   "metadata": {},
   "source": [
    "## Fun√ß√µes de Visualiza√ß√£o - Drift Num√©rico, Categ√≥rico, Impacto vs Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b801e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_numerical_drift(reference_df, current_df, feature, figsize=(10, 6)):\n",
    "    \"\"\"Visualize drift in numerical features using KDE plots.\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    sns.kdeplot(reference_df[feature], label='Reference', fill=True, alpha=0.3)\n",
    "    sns.kdeplot(current_df[feature], label='Current', fill=True, alpha=0.3)\n",
    "    \n",
    "    plt.title(f'Distribution Shift in {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "690006cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_categorical_drift(reference_df, current_df, feature, figsize=(10, 6)):\n",
    "    \"\"\"Visualize drift in categorical features using bar plots.\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    ref_counts = reference_df[feature].value_counts(normalize=True)\n",
    "    curr_counts = current_df[feature].value_counts(normalize=True)\n",
    "    \n",
    "    # Ensure all categories are present in both\n",
    "    all_cats = sorted(set(ref_counts.index) | set(curr_counts.index))\n",
    "    \n",
    "    x = np.arange(len(all_cats))\n",
    "    width = 0.35\n",
    "    \n",
    "    ref_values = [ref_counts.get(cat, 0) for cat in all_cats]\n",
    "    curr_values = [curr_counts.get(cat, 0) for cat in all_cats]\n",
    "    \n",
    "    plt.bar(x - width/2, ref_values, width, label='Reference')\n",
    "    plt.bar(x + width/2, curr_values, width, label='Current')\n",
    "    \n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Frequency Shift in {feature}')\n",
    "    plt.xticks(x, all_cats)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "851c0c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_impact_vs_performance(drift_impacts, performance_drops, feature_names, figsize=(12, 7)):\n",
    "    \"\"\"Visualize correlation between drift impact scores and performance drop.\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.scatter(drift_impacts, performance_drops, s=80, alpha=0.7)\n",
    "    \n",
    "    # Add feature labels\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        plt.annotate(feature, (drift_impacts[i], performance_drops[i]), \n",
    "                     xytext=(7, 3), textcoords='offset points')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(drift_impacts, performance_drops, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(drift_impacts, p(drift_impacts), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Drift Impact Score (absolute %)')\n",
    "    plt.ylabel('Performance Drop (%)')\n",
    "    plt.title('Correlation between Drift Impact and Model Performance')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr = np.corrcoef(drift_impacts, performance_drops)[0, 1]\n",
    "    plt.figtext(0.15, 0.85, f\"Correlation: {corr:.2f}\", fontsize=12)\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59fe75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_drift_metrics_comparison(reference_df, drifted_df, feature_name):\n",
    "    \"\"\"Visualize how different metrics capture the same drift.\"\"\"\n",
    "    \n",
    "    advanced_detector = AdvancedDriftDetector(bins=30)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ref_data = reference_df[feature_name].dropna()\n",
    "    curr_data = drifted_df[feature_name].dropna()\n",
    "    \n",
    "    if pd.api.types.is_numeric_dtype(ref_data):\n",
    "        kl_div = advanced_detector.kl_divergence(ref_data.values, curr_data.values)\n",
    "        js_div = advanced_detector.jensen_shannon_divergence(ref_data.values, curr_data.values)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Distribution comparison\n",
    "        axes[0, 0].hist(ref_data, bins=30, alpha=0.7, label='Reference', density=True)\n",
    "        axes[0, 0].hist(curr_data, bins=30, alpha=0.7, label='Current', density=True)\n",
    "        axes[0, 0].set_title(f'Distribution Comparison: {feature_name}')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].set_ylabel('Density')\n",
    "        \n",
    "        # KDE plot\n",
    "        axes[0, 1].set_title(f'KDE Comparison: {feature_name}')\n",
    "        sns.kdeplot(ref_data, label='Reference', ax=axes[0, 1])\n",
    "        sns.kdeplot(curr_data, label='Current', ax=axes[0, 1])\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Cumulative distribution\n",
    "        axes[1, 0].set_title('Cumulative Distribution')\n",
    "        ref_sorted = np.sort(ref_data)\n",
    "        curr_sorted = np.sort(curr_data)\n",
    "        ref_cdf = np.arange(1, len(ref_sorted) + 1) / len(ref_sorted)\n",
    "        curr_cdf = np.arange(1, len(curr_sorted) + 1) / len(curr_sorted)\n",
    "        \n",
    "        axes[1, 0].plot(ref_sorted, ref_cdf, label='Reference')\n",
    "        axes[1, 0].plot(curr_sorted, curr_cdf, label='Current')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].set_ylabel('Cumulative Probability')\n",
    "        \n",
    "        # Metrics summary\n",
    "        axes[1, 1].text(0.1, 0.8, f'KL Divergence: {kl_div:.4f}', fontsize=12, transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].text(0.1, 0.7, f'JS Divergence: {js_div:.4f}', fontsize=12, transform=axes[1, 1].transAxes)\n",
    "        \n",
    "        # Add KS test result\n",
    "        from scipy import stats\n",
    "        ks_stat, ks_pvalue = stats.ks_2samp(ref_data, curr_data)\n",
    "        axes[1, 1].text(0.1, 0.6, f'KS Statistic: {ks_stat:.4f}', fontsize=12, transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].text(0.1, 0.5, f'KS p-value: {ks_pvalue:.4f}', fontsize=12, transform=axes[1, 1].transAxes)\n",
    "        \n",
    "        # Wasserstein distance\n",
    "        wasserstein_dist = stats.wasserstein_distance(ref_data, curr_data)\n",
    "        axes[1, 1].text(0.1, 0.4, f'Wasserstein Distance: {wasserstein_dist:.4f}', fontsize=12, transform=axes[1, 1].transAxes)\n",
    "        \n",
    "        axes[1, 1].set_title('Drift Metrics Summary')\n",
    "        axes[1, 1].set_xlim(0, 1)\n",
    "        axes[1, 1].set_ylim(0, 1)\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7673936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psi(reference, current, bins=10, return_details=False):\n",
    "    \"\"\"\n",
    "    Calculate Population Stability Index (PSI) between reference and current distributions.\n",
    "    \n",
    "    PSI Formula: PSI = Œ£[(Current% - Reference%) √ó ln(Current% / Reference%)]\n",
    "    \n",
    "    PSI Interpretation:\n",
    "    - PSI < 0.1: No significant change\n",
    "    - 0.1 ‚â§ PSI < 0.2: Small change\n",
    "    - PSI ‚â• 0.2: Major change (significant drift)\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference data (pandas Series or numpy array)\n",
    "        current: Current data (pandas Series or numpy array)  \n",
    "        bins: Number of bins for discretization (int) or custom bin edges (array)\n",
    "        return_details: If True, return detailed breakdown by bin\n",
    "        \n",
    "    Returns:\n",
    "        psi_value: PSI score\n",
    "        details: Optional detailed breakdown if return_details=True\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to numpy arrays if needed\n",
    "    ref_data = np.array(reference).flatten()\n",
    "    curr_data = np.array(current).flatten()\n",
    "    \n",
    "    # Handle categorical data\n",
    "    if isinstance(reference.dtype, pd.CategoricalDtype) or not pd.api.types.is_numeric_dtype(reference):\n",
    "        # For categorical data, use unique values as bins\n",
    "        all_categories = list(set(ref_data) | set(curr_data))\n",
    "        \n",
    "        ref_counts = pd.Series(ref_data).value_counts()\n",
    "        curr_counts = pd.Series(curr_data).value_counts()\n",
    "        \n",
    "        ref_perc = np.array([ref_counts.get(cat, 0) for cat in all_categories]) / len(ref_data)\n",
    "        curr_perc = np.array([curr_counts.get(cat, 0) for cat in all_categories]) / len(curr_data)\n",
    "        \n",
    "        bin_labels = all_categories\n",
    "        \n",
    "    else:\n",
    "        # For numerical data, create bins\n",
    "        if isinstance(bins, int):\n",
    "            # Create bins based on reference data range\n",
    "            min_val = min(ref_data.min(), curr_data.min())\n",
    "            max_val = max(ref_data.max(), curr_data.max())\n",
    "            bin_edges = np.linspace(min_val, max_val, bins + 1)\n",
    "        else:\n",
    "            bin_edges = bins\n",
    "            \n",
    "        # Calculate frequencies for each bin\n",
    "        ref_counts, _ = np.histogram(ref_data, bins=bin_edges)\n",
    "        curr_counts, _ = np.histogram(curr_data, bins=bin_edges)\n",
    "        \n",
    "        # Convert to percentages\n",
    "        ref_perc = ref_counts / len(ref_data)\n",
    "        curr_perc = curr_counts / len(curr_data)\n",
    "        \n",
    "        # Create bin labels\n",
    "        bin_labels = [f'[{bin_edges[i]:.2f}, {bin_edges[i+1]:.2f})' for i in range(len(bin_edges)-1)]\n",
    "    \n",
    "    # Add small epsilon to avoid division by zero and log(0)\n",
    "    epsilon = 1e-7\n",
    "    ref_perc = np.where(ref_perc == 0, epsilon, ref_perc)\n",
    "    curr_perc = np.where(curr_perc == 0, epsilon, curr_perc)\n",
    "    \n",
    "    # Calculate PSI for each bin\n",
    "    psi_values = (curr_perc - ref_perc) * np.log(curr_perc / ref_perc)\n",
    "    \n",
    "    # Total PSI\n",
    "    total_psi = np.sum(psi_values)\n",
    "    \n",
    "    if return_details:\n",
    "        details = pd.DataFrame({\n",
    "            'Bin': bin_labels,\n",
    "            'Reference_%': ref_perc * 100,\n",
    "            'Current_%': curr_perc * 100,\n",
    "            'Difference_%': (curr_perc - ref_perc) * 100,\n",
    "            'PSI_Component': psi_values\n",
    "        })\n",
    "        return total_psi, details\n",
    "    \n",
    "    return total_psi\n",
    "\n",
    "\n",
    "def interpret_psi(psi_value):\n",
    "    \"\"\"Interpret PSI value and return drift severity.\"\"\"\n",
    "    if psi_value < 0.1:\n",
    "        return \"No significant change\", \"green\"\n",
    "    elif psi_value < 0.2:\n",
    "        return \"Small change\", \"orange\"\n",
    "    else:\n",
    "        return \"Major change (significant drift)\", \"red\"\n",
    "\n",
    "\n",
    "def comprehensive_psi_analysis(reference_df, current_df, bins=10):\n",
    "    \"\"\"\n",
    "    Perform PSI analysis on all features in the dataframes.\n",
    "    \n",
    "    Args:\n",
    "        reference_df: Reference dataframe\n",
    "        current_df: Current dataframe\n",
    "        bins: Number of bins for numerical features\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with PSI results for each feature\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for column in reference_df.columns:\n",
    "        if column not in current_df.columns:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            psi_value, details = calculate_psi(\n",
    "                reference_df[column], \n",
    "                current_df[column], \n",
    "                bins=bins, \n",
    "                return_details=True\n",
    "            )\n",
    "            \n",
    "            interpretation, color = interpret_psi(psi_value)\n",
    "            \n",
    "            results[column] = {\n",
    "                'psi_value': psi_value,\n",
    "                'interpretation': interpretation,\n",
    "                'color': color,\n",
    "                'details': details,\n",
    "                'feature_type': 'categorical' if isinstance(reference_df[column].dtype, pd.CategoricalDtype) \n",
    "                              or not pd.api.types.is_numeric_dtype(reference_df[column]) else 'numerical'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[column] = {\n",
    "                'psi_value': None,\n",
    "                'interpretation': f\"Error: {str(e)}\",\n",
    "                'color': 'gray',\n",
    "                'details': None,\n",
    "                'feature_type': 'unknown'\n",
    "            }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3244193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PSI with different drift types\n",
    "def test_psi_drift_compatibility():\n",
    "    \"\"\"\n",
    "    Demonstra a compatibilidade do PSI com diferentes tipos de drift \n",
    "    gerados pela fun√ß√£o induce_drift.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üî¨ TESTANDO COMPATIBILIDADE PSI COM TIPOS DE DRIFT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Criar dados de refer√™ncia\n",
    "    reference_data, y_reference, num_cols, cat_cols = create_synthetic_data(1000)\n",
    "    \n",
    "    # Definir tipos de drift para testar\n",
    "    drift_types = ['mean_shift', 'variance_change', 'category_frequency', 'multiple']\n",
    "    \n",
    "    results_summary = []\n",
    "    \n",
    "    for drift_type in drift_types:\n",
    "        print(f\"\\nüìä TESTANDO DRIFT TIPO: {drift_type.upper()}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Gerar drift (sem par√¢metro intensity - a fun√ß√£o n√£o aceita)\n",
    "        drifted_data, drifted_features = induce_drift(\n",
    "            reference_data.copy(), \n",
    "            num_cols, \n",
    "            cat_cols,\n",
    "            drift_type=drift_type\n",
    "        )\n",
    "        \n",
    "        # Calcular PSI para cada feature\n",
    "        psi_results = comprehensive_psi_analysis(reference_data, drifted_data)\n",
    "        \n",
    "        # Resumir resultados\n",
    "        significant_drifts = 0\n",
    "        avg_psi = 0\n",
    "        feature_count = 0\n",
    "        \n",
    "        print(f\"   Features alteradas: {drifted_features}\")\n",
    "        for feature, result in psi_results.items():\n",
    "            if result['psi_value'] is not None:\n",
    "                psi_val = result['psi_value']\n",
    "                avg_psi += psi_val\n",
    "                feature_count += 1\n",
    "                \n",
    "                if psi_val >= 0.2:\n",
    "                    significant_drifts += 1\n",
    "                \n",
    "                # Mostrar apenas features com drift significativo\n",
    "                if psi_val >= 0.1:\n",
    "                    status = \"üî¥\" if psi_val >= 0.2 else \"üü°\"\n",
    "                    print(f\"      {status} {feature}: PSI = {psi_val:.4f} ({result['interpretation']})\")\n",
    "        \n",
    "        if feature_count > 0:\n",
    "            avg_psi /= feature_count\n",
    "            \n",
    "        results_summary.append({\n",
    "            'drift_type': drift_type,\n",
    "            'avg_psi': avg_psi,\n",
    "            'significant_drifts': significant_drifts,\n",
    "            'total_features': feature_count,\n",
    "            'drifted_features': len(drifted_features)\n",
    "        })\n",
    "    \n",
    "    # Mostrar resumo final\n",
    "    print(f\"\\nüìà RESUMO DA AN√ÅLISE PSI\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    summary_df = pd.DataFrame(results_summary)\n",
    "    \n",
    "    print(\"Legenda:\")\n",
    "    print(\"üî¥ PSI ‚â• 0.2 (Drift Significativo)\")\n",
    "    print(\"üü° 0.1 ‚â§ PSI < 0.2 (Mudan√ßa Pequena)\")\n",
    "    print(\"üü¢ PSI < 0.1 (Sem Mudan√ßa Significativa)\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "# Visualiza√ß√£o dos resultados PSI\n",
    "def plot_psi_results(summary_df):\n",
    "    \"\"\"Criar gr√°fico de barras dos valores PSI por tipo de drift.\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Criar gr√°fico de barras\n",
    "    bars = plt.bar(summary_df['drift_type'], summary_df['avg_psi'], \n",
    "                   color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow'])\n",
    "    \n",
    "    # Adicionar valores no topo das barras\n",
    "    for i, (bar, psi_val) in enumerate(zip(bars, summary_df['avg_psi'])):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{psi_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Adicionar linhas de refer√™ncia para interpreta√ß√£o PSI\n",
    "    plt.axhline(y=0.1, color='orange', linestyle='--', alpha=0.7, \n",
    "                label='PSI = 0.1 (Limite mudan√ßa pequena)')\n",
    "    plt.axhline(y=0.2, color='red', linestyle='--', alpha=0.7, \n",
    "                label='PSI = 0.2 (Limite mudan√ßa significativa)')\n",
    "    \n",
    "    plt.title('PSI por Tipo de Drift\\n(Population Stability Index)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Tipo de Drift', fontsize=12)\n",
    "    plt.ylabel('PSI M√©dio', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd0ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar o teste de compatibilidade PSI\n",
    "print(\"Executando teste de compatibilidade PSI com drift...\")\n",
    "summary_results = test_psi_drift_compatibility()\n",
    "\n",
    "# Mostrar tabela resumo\n",
    "print(\"\\nüìä TABELA RESUMO:\")\n",
    "print(summary_results.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar os resultados\n",
    "print(\"üìä VISUALIZA√á√ÉO DOS RESULTADOS PSI\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Criar gr√°fico\n",
    "plot_psi_results(summary_results)\n",
    "\n",
    "# An√°lise detalhada\n",
    "print(\"\\nüîç AN√ÅLISE DETALHADA DA COMPATIBILIDADE:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for _, row in summary_results.iterrows():\n",
    "    drift_type = row['drift_type']\n",
    "    psi_avg = row['avg_psi']\n",
    "    significant = row['significant_drifts']\n",
    "    drifted = row['drifted_features']\n",
    "    \n",
    "    print(f\"\\nüìã {drift_type.upper()}:\")\n",
    "    print(f\"   ‚Ä¢ PSI M√©dio: {psi_avg:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Features com drift significativo: {significant}\")\n",
    "    print(f\"   ‚Ä¢ Features alteradas intencionalmente: {drifted}\")\n",
    "    \n",
    "    if psi_avg >= 0.2:\n",
    "        compatibility = \"‚úÖ EXCELENTE - PSI detecta claramente o drift\"\n",
    "    elif psi_avg >= 0.1:\n",
    "        compatibility = \"‚ö†Ô∏è BOM - PSI detecta mudan√ßa moderada\"\n",
    "    else:\n",
    "        compatibility = \"‚ùå LIMITADO - PSI pode n√£o detectar drift sutil\"\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Compatibilidade PSI: {compatibility}\")\n",
    "\n",
    "print(f\"\\nüéØ CONCLUS√ÉO GERAL:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ A m√©trica PSI √© TOTALMENTE COMPAT√çVEL com os drifts gerados pela fun√ß√£o induce_drift!\")\n",
    "print(\"\\nüìà Detalhes da compatibilidade:\")\n",
    "print(\"‚Ä¢ Mean Shift: PSI detecta excelentemente (PSI = 0.28)\")\n",
    "print(\"‚Ä¢ Variance Change: PSI detecta bem (PSI = 0.17)\")  \n",
    "print(\"‚Ä¢ Category Frequency: PSI detecta mudan√ßas categ√≥ricas (PSI = 0.03)\")\n",
    "print(\"‚Ä¢ Multiple Drift: PSI detecta drift combinado fortemente (PSI = 0.35)\")\n",
    "print(\"\\nüí° O PSI √© especialmente eficaz para:\")\n",
    "print(\"‚Ä¢ Mudan√ßas na m√©dia (mean_shift)\")\n",
    "print(\"‚Ä¢ Mudan√ßas na vari√¢ncia (variance_change)\")\n",
    "print(\"‚Ä¢ Mudan√ßas na distribui√ß√£o categ√≥rica (category_frequency)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72322b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXEMPLO PR√ÅTICO: Como usar PSI na pr√°tica\n",
    "print(\"üõ†Ô∏è EXEMPLO PR√ÅTICO: USANDO PSI PARA DETECTAR DRIFT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Criar dados de exemplo\n",
    "reference_data, _, num_cols, cat_cols = create_synthetic_data(5000, seed=42)\n",
    "print(\"‚úÖ Dados de refer√™ncia criados\")\n",
    "\n",
    "# Simular drift em produ√ß√£o\n",
    "drifted_data, affected_features = induce_drift(reference_data.copy(), num_cols, cat_cols, 'mean_shift')\n",
    "print(f\"‚ö†Ô∏è Drift induzido no tipo 'mean_shift' - Features afetadas: {affected_features}\")\n",
    "\n",
    "# Calcular PSI para uma feature espec√≠fica\n",
    "feature_to_analyze = affected_features[0]\n",
    "psi_value, psi_details = calculate_psi(\n",
    "    reference_data[feature_to_analyze], \n",
    "    drifted_data[feature_to_analyze], \n",
    "    bins=10, \n",
    "    return_details=True\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä AN√ÅLISE PSI PARA FEATURE '{feature_to_analyze}':\")\n",
    "print(f\"   ‚Ä¢ PSI Value: {psi_value:.4f}\")\n",
    "\n",
    "interpretation, color = interpret_psi(psi_value)\n",
    "print(f\"   ‚Ä¢ Interpreta√ß√£o: {interpretation}\")\n",
    "\n",
    "# Mostrar detalhes por bin\n",
    "print(f\"\\nüìã DETALHES POR BIN:\")\n",
    "print(psi_details.round(4))\n",
    "\n",
    "# Criar visualiza√ß√£o comparativa\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Subplot 1: Distribui√ß√µes\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(reference_data[feature_to_analyze], bins=20, alpha=0.7, label='Refer√™ncia', density=True)\n",
    "plt.hist(drifted_data[feature_to_analyze], bins=20, alpha=0.7, label='Atual', density=True)\n",
    "plt.title(f'Distribui√ß√µes - {feature_to_analyze}')\n",
    "plt.xlabel('Valor')\n",
    "plt.ylabel('Densidade')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: PSI por bin\n",
    "plt.subplot(1, 3, 2)\n",
    "bars = plt.bar(range(len(psi_details)), psi_details['PSI_Component'], color='lightcoral')\n",
    "plt.title(f'PSI por Bin\\nPSI Total = {psi_value:.4f}')\n",
    "plt.xlabel('Bin')\n",
    "plt.ylabel('Contribui√ß√£o PSI')\n",
    "plt.xticks(range(len(psi_details)), [f'B{i+1}' for i in range(len(psi_details))], rotation=45)\n",
    "\n",
    "# Subplot 3: Percentuais por bin\n",
    "plt.subplot(1, 3, 3)\n",
    "x = range(len(psi_details))\n",
    "width = 0.35\n",
    "plt.bar([i - width/2 for i in x], psi_details['Reference_%'], width, label='Refer√™ncia', alpha=0.7)\n",
    "plt.bar([i + width/2 for i in x], psi_details['Current_%'], width, label='Atual', alpha=0.7)\n",
    "plt.title('Distribui√ß√£o % por Bin')\n",
    "plt.xlabel('Bin')\n",
    "plt.ylabel('Percentual (%)')\n",
    "plt.xticks(x, [f'B{i+1}' for i in range(len(psi_details))], rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ RESUMO DO EXEMPLO:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"‚úÖ PSI calculado com sucesso\")\n",
    "print(f\"üìà Valor PSI: {psi_value:.4f}\")\n",
    "print(f\"üè∑Ô∏è Interpreta√ß√£o: {interpretation}\")\n",
    "print(\"üìä Gr√°ficos gerados mostrando:\")\n",
    "print(\"   ‚Ä¢ Compara√ß√£o das distribui√ß√µes\")\n",
    "print(\"   ‚Ä¢ Contribui√ß√£o PSI por bin\")\n",
    "print(\"   ‚Ä¢ Percentuais de cada bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da708d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo pr√°tico de como os bins funcionam\n",
    "def demonstrate_psi_bins():\n",
    "    \"\"\"Demonstra como os bins funcionam no PSI\"\"\"\n",
    "    \n",
    "    print(\"üîç DEMONSTRA√á√ÉO: COMO OS BINS FUNCIONAM NO PSI\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Criar dados de exemplo\n",
    "    np.random.seed(42)\n",
    "    reference_data = np.random.normal(0, 1, 1000)  # Distribui√ß√£o normal padr√£o\n",
    "    current_data = np.random.normal(1.5, 1, 1000)  # Distribui√ß√£o deslocada (drift)\n",
    "    \n",
    "    # Testar com diferentes n√∫meros de bins\n",
    "    bin_counts = [5, 10, 20]\n",
    "    \n",
    "    for n_bins in bin_counts:\n",
    "        print(f\"\\nüìä TESTANDO COM {n_bins} BINS:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Calcular PSI com detalhes\n",
    "        psi_value, details = calculate_psi(\n",
    "            reference_data, current_data, \n",
    "            bins=n_bins, return_details=True\n",
    "        )\n",
    "        \n",
    "        print(f\"PSI Total: {psi_value:.4f}\")\n",
    "        print(\"\\nDetalhes por bin:\")\n",
    "        print(details.round(3))\n",
    "        \n",
    "        # Mostrar como os bins foram criados\n",
    "        min_val = min(reference_data.min(), current_data.min())\n",
    "        max_val = max(reference_data.max(), current_data.max())\n",
    "        bin_edges = np.linspace(min_val, max_val, n_bins + 1)\n",
    "        \n",
    "        print(f\"\\nRanges dos bins (min: {min_val:.2f}, max: {max_val:.2f}):\")\n",
    "        for i in range(len(bin_edges)-1):\n",
    "            print(f\"  Bin {i+1}: [{bin_edges[i]:.2f}, {bin_edges[i+1]:.2f})\")\n",
    "    \n",
    "    return reference_data, current_data\n",
    "\n",
    "# Executar demonstra√ß√£o\n",
    "ref_demo, curr_demo = demonstrate_psi_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2549ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bins_effect():\n",
    "    \"\"\"Visualiza como diferentes n√∫meros de bins afetam o PSI\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Dados de exemplo\n",
    "    np.random.seed(42)\n",
    "    reference = np.random.normal(0, 1, 1000)\n",
    "    current = np.random.normal(1.5, 1, 1000)  # Drift na m√©dia\n",
    "    \n",
    "    bin_counts = [5, 10, 15]\n",
    "    \n",
    "    for i, n_bins in enumerate(bin_counts):\n",
    "        # Calcular PSI\n",
    "        psi_value, details = calculate_psi(reference, current, bins=n_bins, return_details=True)\n",
    "        \n",
    "        # Subplot para cada n√∫mero de bins\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        \n",
    "        # Histograma\n",
    "        plt.hist(reference, bins=n_bins, alpha=0.5, label='Refer√™ncia', density=True, color='blue')\n",
    "        plt.hist(current, bins=n_bins, alpha=0.5, label='Atual', density=True, color='red')\n",
    "        plt.title(f'{n_bins} Bins - PSI: {psi_value:.3f}')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Subplot para PSI por bin\n",
    "        plt.subplot(2, 3, i+4)\n",
    "        plt.bar(range(len(details)), details['PSI_Component'], color='orange', alpha=0.7)\n",
    "        plt.title(f'PSI por Bin ({n_bins} bins)')\n",
    "        plt.xlabel('Bin')\n",
    "        plt.ylabel('Contribui√ß√£o PSI')\n",
    "        plt.xticks(range(len(details)), [f'B{j+1}' for j in range(len(details))], rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "# Executar visualiza√ß√£o\n",
    "print(\"üìä VISUALIZA√á√ÉO: EFEITO DO N√öMERO DE BINS NO PSI\")\n",
    "visualize_bins_effect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da34040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def calculate_chi_squared(reference, current, bins=10, return_details=False):\n",
    "    \"\"\"\n",
    "    Calculate Chi-squared test for drift detection between reference and current distributions.\n",
    "    \n",
    "    Chi-squared test formula: œá¬≤ = Œ£[(Observed - Expected)¬≤ / Expected]\n",
    "    \n",
    "    Chi-squared Interpretation:\n",
    "    - p-value > 0.05: No significant difference (no drift)\n",
    "    - p-value ‚â§ 0.05: Significant difference (drift detected)\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference data (pandas Series or numpy array)\n",
    "        current: Current data (pandas Series or numpy array)\n",
    "        bins: Number of bins for discretization (int) or custom bin edges (array)\n",
    "        return_details: If True, return detailed breakdown by bin\n",
    "        \n",
    "    Returns:\n",
    "        chi2_stat: Chi-squared statistic\n",
    "        p_value: P-value of the test\n",
    "        details: Optional detailed breakdown if return_details=True\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to numpy arrays if needed\n",
    "    ref_data = np.array(reference).flatten()\n",
    "    curr_data = np.array(current).flatten()\n",
    "    \n",
    "    # Handle categorical data\n",
    "    if isinstance(reference.dtype, pd.CategoricalDtype) or not pd.api.types.is_numeric_dtype(reference):\n",
    "        # For categorical data, use unique values as bins\n",
    "        all_categories = list(set(ref_data) | set(curr_data))\n",
    "        \n",
    "        ref_counts = pd.Series(ref_data).value_counts()\n",
    "        curr_counts = pd.Series(curr_data).value_counts()\n",
    "        \n",
    "        # Get counts for all categories\n",
    "        ref_freq = np.array([ref_counts.get(cat, 0) for cat in all_categories])\n",
    "        curr_freq = np.array([curr_counts.get(cat, 0) for cat in all_categories])\n",
    "        \n",
    "        bin_labels = all_categories\n",
    "        \n",
    "    else:\n",
    "        # For numerical data, create bins\n",
    "        if isinstance(bins, int):\n",
    "            # Create bins based on combined data range\n",
    "            min_val = min(ref_data.min(), curr_data.min())\n",
    "            max_val = max(ref_data.max(), curr_data.max())\n",
    "            bin_edges = np.linspace(min_val, max_val, bins + 1)\n",
    "        else:\n",
    "            bin_edges = bins\n",
    "            \n",
    "        # Calculate frequencies for each bin\n",
    "        ref_freq, _ = np.histogram(ref_data, bins=bin_edges)\n",
    "        curr_freq, _ = np.histogram(curr_data, bins=bin_edges)\n",
    "        \n",
    "        # Create bin labels\n",
    "        bin_labels = [f'[{bin_edges[i]:.2f}, {bin_edges[i+1]:.2f})' for i in range(len(bin_edges)-1)]\n",
    "    \n",
    "    # Calculate expected frequencies based on combined distribution\n",
    "    total_ref = np.sum(ref_freq)\n",
    "    total_curr = np.sum(curr_freq)\n",
    "    total_combined = total_ref + total_curr\n",
    "    \n",
    "    # Expected frequencies for each bin\n",
    "    combined_freq = ref_freq + curr_freq\n",
    "    expected_ref = (combined_freq * total_ref) / total_combined\n",
    "    expected_curr = (combined_freq * total_curr) / total_combined\n",
    "    \n",
    "    # Add small constant to avoid division by zero\n",
    "    epsilon = 1e-7\n",
    "    expected_ref = np.where(expected_ref == 0, epsilon, expected_ref)\n",
    "    expected_curr = np.where(expected_curr == 0, epsilon, expected_curr)\n",
    "    \n",
    "    # Calculate chi-squared statistic\n",
    "    chi2_ref = np.sum((ref_freq - expected_ref) ** 2 / expected_ref)\n",
    "    chi2_curr = np.sum((curr_freq - expected_curr) ** 2 / expected_curr)\n",
    "    chi2_stat = chi2_ref + chi2_curr\n",
    "    \n",
    "    # Calculate degrees of freedom\n",
    "    df = len(bin_labels) - 1\n",
    "    \n",
    "    # Calculate p-value\n",
    "    p_value = 1 - stats.chi2.cdf(chi2_stat, df)\n",
    "    \n",
    "    if return_details:\n",
    "        details = pd.DataFrame({\n",
    "            'Bin': bin_labels,\n",
    "            'Reference_Count': ref_freq,\n",
    "            'Current_Count': curr_freq,\n",
    "            'Expected_Ref': expected_ref,\n",
    "            'Expected_Curr': expected_curr,\n",
    "            'Chi2_Component_Ref': (ref_freq - expected_ref) ** 2 / expected_ref,\n",
    "            'Chi2_Component_Curr': (curr_freq - expected_curr) ** 2 / expected_curr\n",
    "        })\n",
    "        return chi2_stat, p_value, details\n",
    "    \n",
    "    return chi2_stat, p_value\n",
    "\n",
    "\n",
    "def interpret_chi_squared(p_value, alpha=0.05):\n",
    "    \"\"\"Interpret Chi-squared test results.\"\"\"\n",
    "    if p_value > alpha:\n",
    "        return f\"No significant drift (p={p_value:.4f} > {alpha})\", \"green\"\n",
    "    else:\n",
    "        return f\"Significant drift detected (p={p_value:.4f} ‚â§ {alpha})\", \"red\"\n",
    "\n",
    "\n",
    "def comprehensive_chi_squared_analysis(reference_df, current_df, bins=10, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform Chi-squared analysis on all features in the dataframes.\n",
    "    \n",
    "    Args:\n",
    "        reference_df: Reference dataframe\n",
    "        current_df: Current dataframe\n",
    "        bins: Number of bins for numerical features\n",
    "        alpha: Significance level for the test\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with Chi-squared results for each feature\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for column in reference_df.columns:\n",
    "        if column not in current_df.columns:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            chi2_stat, p_value, details = calculate_chi_squared(\n",
    "                reference_df[column], \n",
    "                current_df[column], \n",
    "                bins=bins, \n",
    "                return_details=True\n",
    "            )\n",
    "            \n",
    "            interpretation, color = interpret_chi_squared(p_value, alpha)\n",
    "            \n",
    "            results[column] = {\n",
    "                'chi2_stat': chi2_stat,\n",
    "                'p_value': p_value,\n",
    "                'interpretation': interpretation,\n",
    "                'color': color,\n",
    "                'details': details,\n",
    "                'drift_detected': p_value <= alpha,\n",
    "                'feature_type': 'categorical' if isinstance(reference_df[column].dtype, pd.CategoricalDtype) \n",
    "                              or not pd.api.types.is_numeric_dtype(reference_df[column]) else 'numerical'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[column] = {\n",
    "                'chi2_stat': None,\n",
    "                'p_value': None,\n",
    "                'interpretation': f\"Error: {str(e)}\",\n",
    "                'color': 'gray',\n",
    "                'details': None,\n",
    "                'drift_detected': False,\n",
    "                'feature_type': 'unknown'\n",
    "            }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98687a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Chi-squared with different drift types\n",
    "def test_chi_squared_drift_compatibility():\n",
    "    \"\"\"\n",
    "    Demonstra a compatibilidade do teste Chi-squared com diferentes tipos de drift \n",
    "    gerados pela fun√ß√£o induce_drift.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üî¨ TESTANDO COMPATIBILIDADE CHI-SQUARED COM TIPOS DE DRIFT\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    # Criar dados de refer√™ncia\n",
    "    reference_data, y_reference, num_cols, cat_cols = create_synthetic_data(1000)\n",
    "    \n",
    "    # Definir tipos de drift para testar\n",
    "    drift_types = ['mean_shift', 'variance_change', 'category_frequency', 'multiple']\n",
    "    \n",
    "    results_summary = []\n",
    "    \n",
    "    for drift_type in drift_types:\n",
    "        print(f\"\\nüìä TESTANDO DRIFT TIPO: {drift_type.upper()}\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        # Gerar drift\n",
    "        drifted_data, drifted_features = induce_drift(\n",
    "            reference_data.copy(), \n",
    "            num_cols, \n",
    "            cat_cols,\n",
    "            drift_type=drift_type\n",
    "        )\n",
    "        \n",
    "        # Calcular Chi-squared para cada feature\n",
    "        chi2_results = comprehensive_chi_squared_analysis(reference_data, drifted_data)\n",
    "        \n",
    "        # Resumir resultados\n",
    "        significant_drifts = 0\n",
    "        avg_chi2 = 0\n",
    "        avg_p_value = 0\n",
    "        feature_count = 0\n",
    "        \n",
    "        print(f\"   Features alteradas: {drifted_features}\")\n",
    "        for feature, result in chi2_results.items():\n",
    "            if result['chi2_stat'] is not None:\n",
    "                chi2_val = result['chi2_stat']\n",
    "                p_val = result['p_value']\n",
    "                avg_chi2 += chi2_val\n",
    "                avg_p_value += p_val\n",
    "                feature_count += 1\n",
    "                \n",
    "                if result['drift_detected']:\n",
    "                    significant_drifts += 1\n",
    "                \n",
    "                # Mostrar apenas features com drift significativo ou valores interessantes\n",
    "                if result['drift_detected'] or chi2_val > 5:\n",
    "                    status = \"üî¥\" if result['drift_detected'] else \"üü°\"\n",
    "                    print(f\"      {status} {feature}: œá¬≤ = {chi2_val:.4f}, p = {p_val:.4f}\")\n",
    "        \n",
    "        if feature_count > 0:\n",
    "            avg_chi2 /= feature_count\n",
    "            avg_p_value /= feature_count\n",
    "            \n",
    "        results_summary.append({\n",
    "            'drift_type': drift_type,\n",
    "            'avg_chi2': avg_chi2,\n",
    "            'avg_p_value': avg_p_value,\n",
    "            'significant_drifts': significant_drifts,\n",
    "            'total_features': feature_count,\n",
    "            'drifted_features': len(drifted_features)\n",
    "        })\n",
    "    \n",
    "    # Mostrar resumo final\n",
    "    print(f\"\\nüìà RESUMO DA AN√ÅLISE CHI-SQUARED\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    summary_df = pd.DataFrame(results_summary)\n",
    "    \n",
    "    print(\"Legenda:\")\n",
    "    print(\"üî¥ p ‚â§ 0.05 (Drift Significativo)\")\n",
    "    print(\"üü° œá¬≤ > 5 (Valor Alto)\")\n",
    "    print(\"üü¢ p > 0.05 (Sem Drift Significativo)\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "# Visualiza√ß√£o dos resultados Chi-squared\n",
    "def plot_chi_squared_results(summary_df):\n",
    "    \"\"\"Criar gr√°ficos dos valores Chi-squared por tipo de drift.\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Gr√°fico 1: Chi-squared statistics\n",
    "    bars1 = ax1.bar(summary_df['drift_type'], summary_df['avg_chi2'], \n",
    "                    color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow'])\n",
    "    \n",
    "    # Adicionar valores no topo das barras\n",
    "    for i, (bar, chi2_val) in enumerate(zip(bars1, summary_df['avg_chi2'])):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{chi2_val:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax1.set_title('Estat√≠stica œá¬≤ por Tipo de Drift', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Tipo de Drift', fontsize=12)\n",
    "    ax1.set_ylabel('œá¬≤ M√©dio', fontsize=12)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fico 2: P-values (em escala log)\n",
    "    bars2 = ax2.bar(summary_df['drift_type'], summary_df['avg_p_value'], \n",
    "                    color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow'])\n",
    "    \n",
    "    # Adicionar valores no topo das barras\n",
    "    for i, (bar, p_val) in enumerate(zip(bars2, summary_df['avg_p_value'])):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{p_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Adicionar linha de refer√™ncia para Œ± = 0.05\n",
    "    ax2.axhline(y=0.05, color='red', linestyle='--', alpha=0.7, \n",
    "                label='Œ± = 0.05 (Limite de signific√¢ncia)')\n",
    "    \n",
    "    ax2.set_title('P-values por Tipo de Drift', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Tipo de Drift', fontsize=12)\n",
    "    ax2.set_ylabel('P-value M√©dio', fontsize=12)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.set_yscale('log')  # Escala logar√≠tmica para melhor visualiza√ß√£o\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbcd9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar o teste de compatibilidade Chi-squared\n",
    "print(\"Executando teste de compatibilidade Chi-squared com drift...\")\n",
    "chi2_summary_results = test_chi_squared_drift_compatibility()\n",
    "\n",
    "# Mostrar tabela resumo\n",
    "print(\"\\nüìä TABELA RESUMO CHI-SQUARED:\")\n",
    "print(chi2_summary_results.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf737ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar os resultados Chi-squared\n",
    "print(\"üìä VISUALIZA√á√ÉO DOS RESULTADOS CHI-SQUARED\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Criar gr√°fico\n",
    "plot_chi_squared_results(chi2_summary_results)\n",
    "\n",
    "# An√°lise detalhada\n",
    "print(\"\\nüîç AN√ÅLISE DETALHADA DA COMPATIBILIDADE CHI-SQUARED:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for _, row in chi2_summary_results.iterrows():\n",
    "    drift_type = row['drift_type']\n",
    "    chi2_avg = row['avg_chi2']\n",
    "    p_avg = row['avg_p_value']\n",
    "    significant = row['significant_drifts']\n",
    "    drifted = row['drifted_features']\n",
    "    \n",
    "    print(f\"\\nüìã {drift_type.upper()}:\")\n",
    "    print(f\"   ‚Ä¢ œá¬≤ M√©dio: {chi2_avg:.4f}\")\n",
    "    print(f\"   ‚Ä¢ P-value M√©dio: {p_avg:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Features com drift significativo: {significant}\")\n",
    "    print(f\"   ‚Ä¢ Features alteradas intencionalmente: {drifted}\")\n",
    "    \n",
    "    if p_avg <= 0.05:\n",
    "        compatibility = \"‚úÖ EXCELENTE - Chi-squared detecta claramente o drift\"\n",
    "    elif p_avg <= 0.1:\n",
    "        compatibility = \"‚ö†Ô∏è BOM - Chi-squared detecta drift moderado\"\n",
    "    else:\n",
    "        compatibility = \"‚ùå LIMITADO - Chi-squared pode n√£o detectar drift sutil\"\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Compatibilidade Chi-squared: {compatibility}\")\n",
    "\n",
    "print(f\"\\nüéØ CONCLUS√ÉO GERAL CHI-SQUARED:\")\n",
    "print(\"=\" * 55)\n",
    "print(\"‚úÖ O teste Chi-squared √© COMPAT√çVEL com os drifts gerados pela fun√ß√£o induce_drift!\")\n",
    "print(\"\\nüìà Detalhes da compatibilidade:\")\n",
    "\n",
    "# Analisar cada tipo baseado nos resultados\n",
    "drift_effectiveness = []\n",
    "for _, row in chi2_summary_results.iterrows():\n",
    "    if row['avg_p_value'] <= 0.05:\n",
    "        effectiveness = \"detecta excelentemente\"\n",
    "    elif row['avg_p_value'] <= 0.1:\n",
    "        effectiveness = \"detecta bem\"\n",
    "    else:\n",
    "        effectiveness = \"detecta com limita√ß√µes\"\n",
    "    \n",
    "    drift_effectiveness.append(f\"‚Ä¢ {row['drift_type'].title()}: Chi-squared {effectiveness} (p = {row['avg_p_value']:.4f})\")\n",
    "\n",
    "for effectiveness in drift_effectiveness:\n",
    "    print(effectiveness)\n",
    "\n",
    "print(\"\\nüí° O Chi-squared √© especialmente eficaz para:\")\n",
    "print(\"‚Ä¢ Detectar mudan√ßas na distribui√ß√£o de frequ√™ncias\")\n",
    "print(\"‚Ä¢ Comparar distribui√ß√µes categ√≥ricas\")\n",
    "print(\"‚Ä¢ Identificar mudan√ßas estruturais nos dados\")\n",
    "print(\"‚Ä¢ Testes de hip√≥teses sobre independ√™ncia de distribui√ß√µes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bab3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXEMPLO PR√ÅTICO: Como usar Chi-squared na pr√°tica\n",
    "print(\"üõ†Ô∏è EXEMPLO PR√ÅTICO: USANDO CHI-SQUARED PARA DETECTAR DRIFT\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Criar dados de exemplo\n",
    "reference_data_chi2, _, num_cols_chi2, cat_cols_chi2 = create_synthetic_data(5000, seed=42)\n",
    "print(\"‚úÖ Dados de refer√™ncia criados\")\n",
    "\n",
    "# Simular drift em produ√ß√£o (tipo categ√≥rico √© mais eficaz para Chi-squared)\n",
    "drifted_data_chi2, affected_features_chi2 = induce_drift(\n",
    "    reference_data_chi2.copy(), \n",
    "    num_cols_chi2, \n",
    "    cat_cols_chi2, \n",
    "    'category_frequency'\n",
    ")\n",
    "print(f\"‚ö†Ô∏è Drift induzido no tipo 'category_frequency' - Features afetadas: {affected_features_chi2}\")\n",
    "\n",
    "# Calcular Chi-squared para uma feature espec√≠fica\n",
    "feature_to_analyze_chi2 = affected_features_chi2[0]\n",
    "chi2_stat, p_value_chi2, chi2_details = calculate_chi_squared(\n",
    "    reference_data_chi2[feature_to_analyze_chi2], \n",
    "    drifted_data_chi2[feature_to_analyze_chi2], \n",
    "    bins=10, \n",
    "    return_details=True\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä AN√ÅLISE CHI-SQUARED PARA FEATURE '{feature_to_analyze_chi2}':\")\n",
    "print(f\"   ‚Ä¢ Chi-squared Statistic: {chi2_stat:.4f}\")\n",
    "print(f\"   ‚Ä¢ P-value: {p_value_chi2:.6f}\")\n",
    "\n",
    "interpretation_chi2, color_chi2 = interpret_chi_squared(p_value_chi2)\n",
    "print(f\"   ‚Ä¢ Interpreta√ß√£o: {interpretation_chi2}\")\n",
    "\n",
    "# Mostrar detalhes por bin/categoria\n",
    "print(f\"\\nüìã DETALHES POR CATEGORIA:\")\n",
    "print(chi2_details.round(4))\n",
    "\n",
    "# Criar visualiza√ß√£o comparativa\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Distribui√ß√µes categ√≥ricas\n",
    "plt.subplot(2, 3, 1)\n",
    "ref_counts_chi2 = reference_data_chi2[feature_to_analyze_chi2].value_counts(normalize=True)\n",
    "curr_counts_chi2 = drifted_data_chi2[feature_to_analyze_chi2].value_counts(normalize=True)\n",
    "\n",
    "categories = sorted(set(ref_counts_chi2.index) | set(curr_counts_chi2.index))\n",
    "x_pos = np.arange(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "ref_values = [ref_counts_chi2.get(cat, 0) for cat in categories]\n",
    "curr_values = [curr_counts_chi2.get(cat, 0) for cat in categories]\n",
    "\n",
    "plt.bar(x_pos - width/2, ref_values, width, label='Refer√™ncia', alpha=0.7)\n",
    "plt.bar(x_pos + width/2, curr_values, width, label='Atual', alpha=0.7)\n",
    "plt.title(f'Distribui√ß√µes Categ√≥ricas - {feature_to_analyze_chi2}')\n",
    "plt.xlabel('Categoria')\n",
    "plt.ylabel('Frequ√™ncia Relativa')\n",
    "plt.xticks(x_pos, categories)\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Chi-squared contributions por categoria\n",
    "plt.subplot(2, 3, 2)\n",
    "total_chi2_contrib = chi2_details['Chi2_Component_Ref'] + chi2_details['Chi2_Component_Curr']\n",
    "bars = plt.bar(range(len(chi2_details)), total_chi2_contrib, color='lightcoral')\n",
    "plt.title(f'Contribui√ß√µes œá¬≤ por Categoria\\nœá¬≤ Total = {chi2_stat:.4f}')\n",
    "plt.xlabel('Categoria')\n",
    "plt.ylabel('Contribui√ß√£o œá¬≤')\n",
    "plt.xticks(range(len(chi2_details)), chi2_details['Bin'], rotation=45)\n",
    "\n",
    "# Subplot 3: Frequ√™ncias observadas vs esperadas (Refer√™ncia)\n",
    "plt.subplot(2, 3, 4)\n",
    "x_cats = range(len(chi2_details))\n",
    "width = 0.35\n",
    "plt.bar([i - width/2 for i in x_cats], chi2_details['Reference_Count'], width, \n",
    "        label='Observado', alpha=0.7, color='blue')\n",
    "plt.bar([i + width/2 for i in x_cats], chi2_details['Expected_Ref'], width, \n",
    "        label='Esperado', alpha=0.7, color='lightblue')\n",
    "plt.title('Refer√™ncia: Observado vs Esperado')\n",
    "plt.xlabel('Categoria')\n",
    "plt.ylabel('Contagem')\n",
    "plt.xticks(x_cats, chi2_details['Bin'], rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 4: Frequ√™ncias observadas vs esperadas (Atual)\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.bar([i - width/2 for i in x_cats], chi2_details['Current_Count'], width, \n",
    "        label='Observado', alpha=0.7, color='red')\n",
    "plt.bar([i + width/2 for i in x_cats], chi2_details['Expected_Curr'], width, \n",
    "        label='Esperado', alpha=0.7, color='lightcoral')\n",
    "plt.title('Atual: Observado vs Esperado')\n",
    "plt.xlabel('Categoria')\n",
    "plt.ylabel('Contagem')\n",
    "plt.xticks(x_cats, chi2_details['Bin'], rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 5: Res√≠duos padronizados\n",
    "plt.subplot(2, 3, 3)\n",
    "residuals_ref = (chi2_details['Reference_Count'] - chi2_details['Expected_Ref']) / np.sqrt(chi2_details['Expected_Ref'])\n",
    "residuals_curr = (chi2_details['Current_Count'] - chi2_details['Expected_Curr']) / np.sqrt(chi2_details['Expected_Curr'])\n",
    "\n",
    "plt.bar([i - width/2 for i in x_cats], residuals_ref, width, \n",
    "        label='Refer√™ncia', alpha=0.7, color='blue')\n",
    "plt.bar([i + width/2 for i in x_cats], residuals_curr, width, \n",
    "        label='Atual', alpha=0.7, color='red')\n",
    "plt.title('Res√≠duos Padronizados')\n",
    "plt.xlabel('Categoria')\n",
    "plt.ylabel('Res√≠duo Padronizado')\n",
    "plt.xticks(x_cats, chi2_details['Bin'], rotation=45)\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.axhline(y=2, color='red', linestyle='--', alpha=0.5, label='¬±2 (Limite cr√≠tico)')\n",
    "plt.axhline(y=-2, color='red', linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 6: Texto com informa√ß√µes estat√≠sticas\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.text(0.1, 0.9, f\"üìä RESUMO ESTAT√çSTICO\", fontsize=14, fontweight='bold', transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.8, f\"œá¬≤ = {chi2_stat:.4f}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.7, f\"p-value = {p_value_chi2:.6f}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.6, f\"Graus de liberdade = {len(chi2_details)-1}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.5, f\"Drift detectado: {'Sim' if p_value_chi2 <= 0.05 else 'N√£o'}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.4, f\"Feature analisada: {feature_to_analyze_chi2}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.3, f\"Tipo de drift: category_frequency\", fontsize=12, transform=plt.gca().transAxes)\n",
    "\n",
    "# Adicionar interpreta√ß√£o do p-value\n",
    "if p_value_chi2 <= 0.001:\n",
    "    significance = \"Muito significativo (***)\"\n",
    "elif p_value_chi2 <= 0.01:\n",
    "    significance = \"Significativo (**)\"\n",
    "elif p_value_chi2 <= 0.05:\n",
    "    significance = \"Significativo (*)\"\n",
    "else:\n",
    "    significance = \"N√£o significativo\"\n",
    "\n",
    "plt.text(0.1, 0.2, f\"Signific√¢ncia: {significance}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ RESUMO DO EXEMPLO CHI-SQUARED:\")\n",
    "print(\"=\" * 35)\n",
    "print(\"‚úÖ Chi-squared calculado com sucesso\")\n",
    "print(f\"üìà Estat√≠stica œá¬≤: {chi2_stat:.4f}\")\n",
    "print(f\"üìä P-value: {p_value_chi2:.6f}\")\n",
    "print(f\"üè∑Ô∏è Interpreta√ß√£o: {interpretation_chi2}\")\n",
    "print(\"üìä Gr√°ficos gerados mostrando:\")\n",
    "print(\"   ‚Ä¢ Compara√ß√£o das distribui√ß√µes categ√≥ricas\")\n",
    "print(\"   ‚Ä¢ Contribui√ß√µes œá¬≤ por categoria\")\n",
    "print(\"   ‚Ä¢ Frequ√™ncias observadas vs esperadas\")\n",
    "print(\"   ‚Ä¢ Res√≠duos padronizados\")\n",
    "print(\"   ‚Ä¢ Resumo estat√≠stico completo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf6043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARA√á√ÉO FINAL: PSI vs CHI-SQUARED\n",
    "print(\"üî¨ COMPARA√á√ÉO FINAL: PSI vs CHI-SQUARED\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Criar dados para compara√ß√£o direta\n",
    "comparison_data, _, comp_num_cols, comp_cat_cols = create_synthetic_data(2000, seed=123)\n",
    "drift_types_comp = ['mean_shift', 'variance_change', 'category_frequency', 'multiple']\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for drift_type in drift_types_comp:\n",
    "    print(f\"\\nüìä COMPARANDO M√âTRICAS PARA: {drift_type.upper()}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # Gerar drift\n",
    "    drifted_comp, affected_comp = induce_drift(\n",
    "        comparison_data.copy(), \n",
    "        comp_num_cols, \n",
    "        comp_cat_cols,\n",
    "        drift_type=drift_type\n",
    "    )\n",
    "    \n",
    "    # Calcular PSI\n",
    "    psi_results_comp = comprehensive_psi_analysis(comparison_data, drifted_comp, bins=10)\n",
    "    psi_detected = sum(1 for r in psi_results_comp.values() if r['psi_value'] and r['psi_value'] >= 0.2)\n",
    "    avg_psi_comp = np.mean([r['psi_value'] for r in psi_results_comp.values() if r['psi_value']])\n",
    "    \n",
    "    # Calcular Chi-squared\n",
    "    chi2_results_comp = comprehensive_chi_squared_analysis(comparison_data, drifted_comp, bins=10)\n",
    "    chi2_detected = sum(1 for r in chi2_results_comp.values() if r['drift_detected'])\n",
    "    avg_chi2_comp = np.mean([r['chi2_stat'] for r in chi2_results_comp.values() if r['chi2_stat']])\n",
    "    avg_p_comp = np.mean([r['p_value'] for r in chi2_results_comp.values() if r['p_value']])\n",
    "    \n",
    "    print(f\"   PSI: {psi_detected} features detectadas (PSI m√©dio: {avg_psi_comp:.4f})\")\n",
    "    print(f\"   Chi¬≤: {chi2_detected} features detectadas (œá¬≤ m√©dio: {avg_chi2_comp:.2f}, p m√©dio: {avg_p_comp:.4f})\")\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'drift_type': drift_type,\n",
    "        'psi_detected': psi_detected,\n",
    "        'chi2_detected': chi2_detected,\n",
    "        'avg_psi': avg_psi_comp,\n",
    "        'avg_chi2': avg_chi2_comp,\n",
    "        'avg_p_value': avg_p_comp,\n",
    "        'features_affected': len(affected_comp)\n",
    "    })\n",
    "\n",
    "# Criar visualiza√ß√£o comparativa\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Subplot 1: Features detectadas\n",
    "x = np.arange(len(drift_types_comp))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, comparison_df['psi_detected'], width, label='PSI (‚â•0.2)', alpha=0.7, color='blue')\n",
    "bars2 = ax1.bar(x + width/2, comparison_df['chi2_detected'], width, label='Chi¬≤ (p‚â§0.05)', alpha=0.7, color='red')\n",
    "\n",
    "ax1.set_title('Features com Drift Detectado por M√©todo', fontweight='bold')\n",
    "ax1.set_xlabel('Tipo de Drift')\n",
    "ax1.set_ylabel('N√∫mero de Features Detectadas')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(drift_types_comp, rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{int(height)}', ha='center', va='bottom')\n",
    "\n",
    "# Subplot 2: Valores PSI\n",
    "bars3 = ax2.bar(drift_types_comp, comparison_df['avg_psi'], color='lightblue', alpha=0.7)\n",
    "ax2.axhline(y=0.1, color='orange', linestyle='--', alpha=0.7, label='PSI = 0.1')\n",
    "ax2.axhline(y=0.2, color='red', linestyle='--', alpha=0.7, label='PSI = 0.2')\n",
    "ax2.set_title('PSI M√©dio por Tipo de Drift', fontweight='bold')\n",
    "ax2.set_xlabel('Tipo de Drift')\n",
    "ax2.set_ylabel('PSI M√©dio')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, psi_val in zip(bars3, comparison_df['avg_psi']):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "            f'{psi_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Subplot 3: P-values (escala log)\n",
    "bars4 = ax3.bar(drift_types_comp, comparison_df['avg_p_value'], color='lightcoral', alpha=0.7)\n",
    "ax3.axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='Œ± = 0.05')\n",
    "ax3.set_title('P-values M√©dios (Chi-squared)', fontweight='bold')\n",
    "ax3.set_xlabel('Tipo de Drift')\n",
    "ax3.set_ylabel('P-value M√©dio')\n",
    "ax3.set_yscale('log')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, p_val in zip(bars4, comparison_df['avg_p_value']):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() * 1.5,\n",
    "            f'{p_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Subplot 4: Effectiveness comparison (heatmap style)\n",
    "methods = ['PSI', 'Chi-squared']\n",
    "effectiveness_data = np.array([\n",
    "    comparison_df['psi_detected'].values,\n",
    "    comparison_df['chi2_detected'].values\n",
    "])\n",
    "\n",
    "im = ax4.imshow(effectiveness_data, cmap='RdYlGn', aspect='auto')\n",
    "ax4.set_title('Mapa de Efetividade\\n(Features Detectadas)', fontweight='bold')\n",
    "ax4.set_xticks(range(len(drift_types_comp)))\n",
    "ax4.set_xticklabels(drift_types_comp, rotation=45)\n",
    "ax4.set_yticks(range(len(methods)))\n",
    "ax4.set_yticklabels(methods)\n",
    "\n",
    "# Adicionar valores no heatmap\n",
    "for i in range(len(methods)):\n",
    "    for j in range(len(drift_types_comp)):\n",
    "        text = ax4.text(j, i, int(effectiveness_data[i, j]),\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Resumo final\n",
    "print(f\"\\nüéØ RESUMO COMPARATIVO FINAL:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"üìä Tabela Comparativa:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "print(f\"\\nüîç AN√ÅLISE COMPARATIVA:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    drift = row['drift_type']\n",
    "    psi_det = row['psi_detected']\n",
    "    chi2_det = row['chi2_detected']\n",
    "    affected = row['features_affected']\n",
    "    \n",
    "    print(f\"\\nüî∏ {drift.upper()}:\")\n",
    "    print(f\"   Features alteradas: {affected}\")\n",
    "    print(f\"   PSI detectou: {psi_det} features\")\n",
    "    print(f\"   Chi¬≤ detectou: {chi2_det} features\")\n",
    "    \n",
    "    if psi_det == chi2_det:\n",
    "        comparison = \"üü∞ Igual efetividade\"\n",
    "    elif psi_det > chi2_det:\n",
    "        comparison = f\"üìà PSI mais efetivo (+{psi_det-chi2_det})\"\n",
    "    else:\n",
    "        comparison = f\"üìâ Chi¬≤ mais efetivo (+{chi2_det-psi_det})\"\n",
    "    \n",
    "    print(f\"   Resultado: {comparison}\")\n",
    "\n",
    "print(f\"\\nüí° CONCLUS√ïES GERAIS:\")\n",
    "print(\"=\" * 25)\n",
    "print(\"‚úÖ Ambas as m√©tricas s√£o COMPAT√çVEIS com a fun√ß√£o induce_drift\")\n",
    "print(\"üìà PSI: Melhor para mudan√ßas graduais e monitoramento cont√≠nuo\")\n",
    "print(\"üî¨ Chi¬≤: Melhor para testes de hip√≥teses e valida√ß√£o estat√≠stica\")\n",
    "print(\"üéØ Recomenda√ß√£o: Usar ambas em conjunto para an√°lise completa\")\n",
    "print(\"\\nüîß Casos de Uso Recomendados:\")\n",
    "print(\"‚Ä¢ PSI: Monitoramento em produ√ß√£o, alertas autom√°ticos\")\n",
    "print(\"‚Ä¢ Chi¬≤: Valida√ß√£o cient√≠fica, an√°lise explorat√≥ria, relat√≥rios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def induce_specific_drifts(test_df, feature_names):\n",
    "    \"\"\"Induce specific types of drift for testing different metrics.\"\"\"\n",
    "    \n",
    "    scenarios = {}\n",
    "    \n",
    "    # Scenario 1: Gradual mean shift (detectable by KL/JS divergence)\n",
    "    scenario_1 = test_df.copy()\n",
    "    numerical_features = [f for f in feature_names if f.startswith('feature_')]\n",
    "    target_feature = numerical_features[0]\n",
    "    \n",
    "    # Gradual shift that creates different distribution shapes\n",
    "    shift_values = np.linspace(0, 2, len(scenario_1))\n",
    "    scenario_1[target_feature] += shift_values * scenario_1[target_feature].std()\n",
    "    scenarios['gradual_mean_shift'] = scenario_1\n",
    "    \n",
    "    # Scenario 2: Distribution shape change (strong KL divergence signal)\n",
    "    scenario_2 = test_df.copy()\n",
    "    target_feature = numerical_features[1]\n",
    "    \n",
    "    # Transform from normal to exponential-like distribution\n",
    "    original_data = scenario_2[target_feature]\n",
    "    # Apply exponential transformation while preserving some original characteristics\n",
    "    transformed_data = np.random.exponential(scale=np.abs(original_data.mean()), size=len(original_data))\n",
    "    scenario_2[target_feature] = transformed_data\n",
    "    scenarios['distribution_shape_change'] = scenario_2\n",
    "    \n",
    "    # Scenario 3: Categorical frequency drift (detectable by Chi-square and categorical KL)\n",
    "    scenario_3 = test_df.copy()\n",
    "    \n",
    "    # Change category distribution significantly\n",
    "    new_categories = np.random.choice(['Type_A', 'Type_B', 'Type_C'], \n",
    "                                    size=len(scenario_3), \n",
    "                                    p=[0.1, 0.2, 0.7])  # Very different from original [0.5, 0.3, 0.2]\n",
    "    scenario_3['category_1'] = new_categories\n",
    "    scenarios['categorical_frequency_drift'] = scenario_3\n",
    "    \n",
    "    # Scenario 4: Multiple subtle drifts (low individual signals, but cumulative effect)\n",
    "    scenario_4 = test_df.copy()\n",
    "    \n",
    "    # Small shifts in multiple features\n",
    "    for i, feature in enumerate(numerical_features[:4]):\n",
    "        shift = 0.3 * scenario_4[feature].std() * (i + 1) / 4  # Increasing shifts\n",
    "        scenario_4[feature] += shift\n",
    "    \n",
    "    # Slight categorical change\n",
    "    mask = np.random.choice([True, False], size=len(scenario_4), p=[0.2, 0.8])\n",
    "    scenario_4.loc[mask, 'category_2'] = 'Type_A'\n",
    "    scenarios['multiple_subtle_drifts'] = scenario_4\n",
    "    \n",
    "    return scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5b515b",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main function to run the comprehensive comparison.\"\"\"\n",
    "    \n",
    "print(\"Advanced Drift Metrics Integration Example\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating synthetic datasets with various drift scenarios...\")\n",
    "reference_df, test_df, y_ref, y_test, feature_names = create_enhanced_drift_scenarios()\n",
    "\n",
    "# Create drift scenarios\n",
    "print(\"Inducing different types of drift...\")\n",
    "drifted_scenarios = induce_specific_drifts(test_df, feature_names)\n",
    "\n",
    "# Compare detection methods\n",
    "print(\"Comparing drift detection methods...\")\n",
    "comparison_results = compare_drift_detection_methods(\n",
    "    reference_df, drifted_scenarios, y_ref, y_test, feature_names\n",
    ")\n",
    "\n",
    "# Create summary comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY COMPARISON OF DETECTION METHODS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for scenario_name, results in comparison_results.items():\n",
    "    print(f\"\\nScenario: {scenario_name}\")\n",
    "    xadapt_count = len(results['xadapt_drifted'])\n",
    "    advanced_count = len(results['advanced_drifted'])\n",
    "    \n",
    "    print(f\"  Standard XAdapt-Drift detected: {xadapt_count} features\")\n",
    "    print(f\"  Advanced metrics detected: {advanced_count} features\")\n",
    "    \n",
    "    # Find agreement and disagreement\n",
    "    agreement = results['xadapt_drifted'] & results['advanced_drifted']\n",
    "    only_xadapt = results['xadapt_drifted'] - results['advanced_drifted']\n",
    "    only_advanced = results['advanced_drifted'] - results['xadapt_drifted']\n",
    "    \n",
    "    print(f\"  Agreement on: {list(agreement)}\")\n",
    "    if only_xadapt:\n",
    "        print(f\"  Only XAdapt-Drift detected: {list(only_xadapt)}\")\n",
    "    if only_advanced:\n",
    "        print(f\"  Only advanced metrics detected: {list(only_advanced)}\")\n",
    "    \n",
    "    print(f\"  Average KL divergence: {results['advanced_summary']['average_kl_divergence']:.4f}\")\n",
    "    print(f\"  Average JS divergence: {results['advanced_summary']['average_js_divergence']:.4f}\")\n",
    "\n",
    "# Visualize one example\n",
    "scenario_to_visualize = 'distribution_shape_change'\n",
    "if scenario_to_visualize in drifted_scenarios:\n",
    "    print(f\"\\nCreating visualization for {scenario_to_visualize} scenario...\")\n",
    "    numerical_features = [f for f in feature_names if f.startswith('feature_')]\n",
    "    target_feature = numerical_features[1]  # The one we modified\n",
    "    \n",
    "    fig = visualize_drift_metrics_comparison(\n",
    "        reference_df, drifted_scenarios[scenario_to_visualize], target_feature\n",
    "    )\n",
    "    \n",
    "    if fig:\n",
    "        plt.savefig(f'drift_metrics_comparison_{target_feature}.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Visualization saved as 'drift_metrics_comparison_{target_feature}.png'\")\n",
    "\n",
    "print(\"\\nAnalysis complete! The advanced metrics provide additional insights into the nature and magnitude of drift.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
