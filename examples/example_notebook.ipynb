{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "357544f5",
   "metadata": {},
   "source": [
    "# Configuração do Ambiente XAdapt-Drift\n",
    "\n",
    "Este notebook demonstra como usar a biblioteca XAdapt-Drift para análise de drift em modelos de Machine Learning.\n",
    "\n",
    "## Configuração do PYTHONPATH\n",
    "\n",
    "Primeiro, vamos adicionar o diretório raiz da biblioteca ao PYTHONPATH para permitir imports diretos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fbf1f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório do projeto: /home/alexandre/Documents/XDrift-Analyzer\n",
      "✅ Diretório do projeto já está no PYTHONPATH\n",
      "✅ XAdapt-Drift importado com sucesso!\n",
      "Localização da biblioteca: /home/alexandre/Documents/XDrift-Analyzer/xadapt_drift/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# Configurar PYTHONPATH para importar a biblioteca XAdapt-Drift\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Obter o diretório raiz do projeto (um nível acima do diretório 'examples')\n",
    "project_root = Path.cwd().parent\n",
    "print(f\"Diretório do projeto: {project_root}\")\n",
    "\n",
    "# Adicionar ao PYTHONPATH se ainda não estiver\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"✅ Adicionado {project_root} ao PYTHONPATH\")\n",
    "else:\n",
    "    print(\"✅ Diretório do projeto já está no PYTHONPATH\")\n",
    "\n",
    "# Verificar se a biblioteca pode ser importada\n",
    "try:\n",
    "    import xadapt_drift\n",
    "    print(\"✅ XAdapt-Drift importado com sucesso!\")\n",
    "    print(f\"Localização da biblioteca: {xadapt_drift.__file__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Erro ao importar XAdapt-Drift: {e}\")\n",
    "    print(\"Verifique se você está executando o notebook do diretório correto.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f893d59",
   "metadata": {},
   "source": [
    "## 🚀 Exemplo Básico de Uso\n",
    "\n",
    "Agora que a biblioteca está configurada, vamos demonstrar um exemplo básico de uso com o padrão adapter que discutimos anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc72af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo prático: Usando o SklearnAdapter com a BaseAdapter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importar classes da nossa biblioteca\n",
    "from xadapt_drift.adapters.sklearn_adapter import SklearnAdapter\n",
    "from xadapt_drift.drift.detector import DriftDetector\n",
    "\n",
    "print(\"🎯 Demonstração do Valor da BaseAdapter\\n\")\n",
    "\n",
    "# 1. Criar dados de exemplo\n",
    "print(\"📊 Criando dados de exemplo...\")\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "feature_names = [f\"feature_{i}\" for i in range(10)]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 2. Treinar modelo sklearn\n",
    "print(\"🤖 Treinando modelo RandomForest...\")\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Criar adapter usando nossa BaseAdapter\n",
    "print(\"🔌 Criando SklearnAdapter...\")\n",
    "adapter = SklearnAdapter(\n",
    "    model=model,\n",
    "    feature_names=feature_names,\n",
    "    validate_model=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Adapter criado: {adapter}\")\n",
    "print(f\"📋 Informações do modelo: {adapter.get_model_info()['model_type']}\")\n",
    "print(f\"📊 Features: {len(adapter.feature_names)} features\")\n",
    "\n",
    "# 4. Usar o adapter (mesma interface independente do framework!)\n",
    "print(\"\\n🔮 Fazendo predições...\")\n",
    "predictions = adapter.predict(X_test)\n",
    "print(f\"✅ Predições realizadas: {predictions.shape}\")\n",
    "\n",
    "# 5. Gerar explicações\n",
    "print(\"\\n🧠 Gerando explicações SHAP...\")\n",
    "try:\n",
    "    explanations = adapter.explain(X_test[:10], method=\"shap\")  # Usar apenas 10 amostras\n",
    "    print(f\"✅ Explicações geradas para {len(explanations)} features\")\n",
    "    \n",
    "    # Mostrar top 3 features mais importantes\n",
    "    sorted_features = sorted(explanations.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"🏆 Top 3 features mais importantes:\")\n",
    "    for i, (feature, importance) in enumerate(sorted_features[:3], 1):\n",
    "        print(f\"   {i}. {feature}: {importance:.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ SHAP não disponível ou erro: {e}\")\n",
    "    print(\"💡 Tentando método de permutation importance...\")\n",
    "    \n",
    "    try:\n",
    "        explanations = adapter.explain(X_test[:10], y_test[:10], method=\"permutation\")\n",
    "        print(f\"✅ Permutation importance gerada para {len(explanations)} features\")\n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Erro também com permutation: {e2}\")\n",
    "\n",
    "print(\"\\n🎉 Exemplo concluído! A BaseAdapter forneceu:\")\n",
    "print(\"   ✓ Interface consistente independente do framework\")\n",
    "print(\"   ✓ Validação automática de entrada\")\n",
    "print(\"   ✓ Logging integrado\")\n",
    "print(\"   ✓ Tratamento de erros robusto\")\n",
    "print(\"   ✓ Metadados do modelo padronizados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b41d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import sys\n",
    "\n",
    "\n",
    "plt.style.use('seaborn-v0_8-pastel')\n",
    "sns.set_palette('pastel')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Import XAdapt-Drift components\n",
    "from xadapt_drift import XAdaptDrift\n",
    "from xadapt_drift.adapters.sklearn_adapter import SklearnAdapter\n",
    "from xadapt_drift.utils.advanced_metrics import AdvancedDriftDetector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1c4e1",
   "metadata": {},
   "source": [
    "### Criação de dados - geração de dados sintéticos e funções de indução de Drift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c68eb580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_data(n_samples=10000, n_cat_features=3, n_num_features=5, seed=42):\n",
    "    \"\"\"Create a synthetic dataset with mixed data types\"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Create numerical features\n",
    "    X_numerical = np.random.randn(n_samples, n_num_features)\n",
    "    \n",
    "    # Create categorical features (3 categories each)\n",
    "    X_categorical = np.random.randint(0, 3, size=(n_samples, n_cat_features))\n",
    "\n",
    "    # Create target based on both numerical and categorical features\n",
    "    y = (0.5 * np.sum(X_numerical[:, :2], axis=1) + \n",
    "         0.8 * (X_categorical[:, 0] == 2).astype(int) - \n",
    "         0.5 * (X_categorical[:, 1] == 0).astype(int) + \n",
    "         0.1 * np.random.randn(n_samples)) > 0\n",
    "    \n",
    "    # Combine features\n",
    "    X = np.hstack([X_numerical, X_categorical])\n",
    "\n",
    "    # Create feature names\n",
    "    numerical_cols = [f'num_{i}' for i in range(n_num_features)]\n",
    "    categorical_cols = [f'cat_{i}' for i in range(n_cat_features)]\n",
    "    feature_names = numerical_cols + categorical_cols\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    \n",
    "    # Convert categorical columns to correct type\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "    return df, y.astype(int), numerical_cols, categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d866545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def induce_drift(df, num_cols, cat_cols, drift_type='mean_shift'):\n",
    "    \"\"\"Induce different types of drift in the dataset.\n",
    "    Args:\n",
    "        df: Original DataFrame\n",
    "        num_cols: List of numerical feature names\n",
    "        cat_cols: List of categorical feature names\n",
    "        drift_type: Type of drift to induce ('mean_shift', 'variance_change', 'category_frequency', 'multiple')\n",
    "    Returns:\n",
    "        drifted_df: DataFrame with induced drift\n",
    "        drifted_features: List of features that were changed\n",
    "    \"\"\"\n",
    "    \n",
    "    drifted_df = df.copy()\n",
    "    \n",
    "    if drift_type == 'mean_shift':\n",
    "        # Shift the mean of the first numerical feature\n",
    "        feature = num_cols[0]\n",
    "        shift = 1.5 * drifted_df[feature].std()\n",
    "        drifted_df[feature] += shift\n",
    "        drifted_features = [feature]\n",
    "        \n",
    "    elif drift_type == 'variance_change':\n",
    "        # Increase the variance of the second numerical feature\n",
    "        feature = num_cols[1]\n",
    "        drifted_df[feature] = drifted_df[feature] * 2.0\n",
    "        drifted_features = [feature]\n",
    "    \n",
    "    elif drift_type == 'category_frequency':\n",
    "        # Change the distribution of a categorical feature\n",
    "        feature = cat_cols[0]\n",
    "        # Find the least common category\n",
    "        least_common = drifted_df[feature].value_counts().idxmin()\n",
    "        # Make it more common by replacing some values\n",
    "        mask = np.random.choice([True, False], size=len(drifted_df), p=[0.4, 0.6])\n",
    "        drifted_df.loc[mask, feature] = least_common\n",
    "        drifted_features = [feature]\n",
    "        \n",
    "    elif drift_type == 'multiple':\n",
    "        # Induce multiple drifts\n",
    "        # Shift mean of first numerical feature\n",
    "        drifted_df[num_cols[0]] += 1.2 * drifted_df[num_cols[0]].std()\n",
    "        # Increase variance of second numerical feature\n",
    "        drifted_df[num_cols[1]] = drifted_df[num_cols[1]] * 1.8\n",
    "        # Change categorical distribution\n",
    "        feature = cat_cols[0]\n",
    "        mask = np.random.choice([True, False], size=len(drifted_df), p=[0.3, 0.7])\n",
    "        drifted_df.loc[mask, feature] = drifted_df[feature].value_counts().idxmin()\n",
    "        drifted_features = [num_cols[0], num_cols[1], cat_cols[0]]\n",
    "    \n",
    "    return drifted_df, drifted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b668f431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def induce_specific_drifts(test_df, feature_names):\n",
    "    \"\"\"Induce specific types of drift for testing different metrics.\"\"\"\n",
    "    \n",
    "    scenarios = {}\n",
    "    \n",
    "    # Scenario 1: Gradual mean shift (detectable by KL/JS divergence)\n",
    "    scenario_1 = test_df.copy()\n",
    "    numerical_features = [f for f in feature_names if f.startswith('feature_')]\n",
    "    target_feature = numerical_features[0]\n",
    "    \n",
    "    # Gradual shift that creates different distribution shapes\n",
    "    shift_values = np.linspace(0, 2, len(scenario_1))\n",
    "    scenario_1[target_feature] += shift_values * scenario_1[target_feature].std()\n",
    "    scenarios['gradual_mean_shift'] = scenario_1\n",
    "    \n",
    "    # Scenario 2: Distribution shape change (strong KL divergence signal)\n",
    "    scenario_2 = test_df.copy()\n",
    "    target_feature = numerical_features[1]\n",
    "    \n",
    "    # Transform from normal to exponential-like distribution\n",
    "    original_data = scenario_2[target_feature]\n",
    "    # Apply exponential transformation while preserving some original characteristics\n",
    "    transformed_data = np.random.exponential(scale=np.abs(original_data.mean()), size=len(original_data))\n",
    "    scenario_2[target_feature] = transformed_data\n",
    "    scenarios['distribution_shape_change'] = scenario_2\n",
    "    \n",
    "    # Scenario 3: Categorical frequency drift (detectable by Chi-square and categorical KL)\n",
    "    scenario_3 = test_df.copy()\n",
    "    \n",
    "    # Change category distribution significantly\n",
    "    new_categories = np.random.choice(['Type_A', 'Type_B', 'Type_C'], \n",
    "                                    size=len(scenario_3), \n",
    "                                    p=[0.1, 0.2, 0.7])  # Very different from original [0.5, 0.3, 0.2]\n",
    "    scenario_3['category_1'] = new_categories\n",
    "    scenarios['categorical_frequency_drift'] = scenario_3\n",
    "    \n",
    "    # Scenario 4: Multiple subtle drifts (low individual signals, but cumulative effect)\n",
    "    scenario_4 = test_df.copy()\n",
    "    \n",
    "    # Small shifts in multiple features\n",
    "    for i, feature in enumerate(numerical_features[:4]):\n",
    "        shift = 0.3 * scenario_4[feature].std() * (i + 1) / 4  # Increasing shifts\n",
    "        scenario_4[feature] += shift\n",
    "    \n",
    "    # Slight categorical change\n",
    "    mask = np.random.choice([True, False], size=len(scenario_4), p=[0.2, 0.8])\n",
    "    scenario_4.loc[mask, 'category_2'] = 'Type_A'\n",
    "    scenarios['multiple_subtle_drifts'] = scenario_4\n",
    "    \n",
    "    return scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4644742",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df, y_ref, numerical_cols, categorical_cols = create_synthetic_data(n_samples=10000)\n",
    "print(f\"Created dataset with {len(numerical_cols)} numerical features and {len(categorical_cols)} categorical features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dbe8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c2065",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863bf3d",
   "metadata": {},
   "source": [
    "## Funções de Visualização - Drift Numérico, Categórico, Impacto vs Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b801e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_numerical_drift(reference_df, current_df, feature, figsize=(10, 6)):\n",
    "    \"\"\"Visualize drift in numerical features using KDE plots.\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    sns.kdeplot(reference_df[feature], label='Reference', fill=True, alpha=0.3)\n",
    "    sns.kdeplot(current_df[feature], label='Current', fill=True, alpha=0.3)\n",
    "    \n",
    "    plt.title(f'Distribution Shift in {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "690006cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_categorical_drift(reference_df, current_df, feature, figsize=(10, 6)):\n",
    "    \"\"\"Visualize drift in categorical features using bar plots.\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    ref_counts = reference_df[feature].value_counts(normalize=True)\n",
    "    curr_counts = current_df[feature].value_counts(normalize=True)\n",
    "    \n",
    "    # Ensure all categories are present in both\n",
    "    all_cats = sorted(set(ref_counts.index) | set(curr_counts.index))\n",
    "    \n",
    "    x = np.arange(len(all_cats))\n",
    "    width = 0.35\n",
    "    \n",
    "    ref_values = [ref_counts.get(cat, 0) for cat in all_cats]\n",
    "    curr_values = [curr_counts.get(cat, 0) for cat in all_cats]\n",
    "    \n",
    "    plt.bar(x - width/2, ref_values, width, label='Reference')\n",
    "    plt.bar(x + width/2, curr_values, width, label='Current')\n",
    "    \n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Frequency Shift in {feature}')\n",
    "    plt.xticks(x, all_cats)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "851c0c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_impact_vs_performance(drift_impacts, performance_drops, feature_names, figsize=(12, 7)):\n",
    "    \"\"\"Visualize correlation between drift impact scores and performance drop.\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.scatter(drift_impacts, performance_drops, s=80, alpha=0.7)\n",
    "    \n",
    "    # Add feature labels\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        plt.annotate(feature, (drift_impacts[i], performance_drops[i]), \n",
    "                     xytext=(7, 3), textcoords='offset points')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(drift_impacts, performance_drops, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(drift_impacts, p(drift_impacts), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Drift Impact Score (absolute %)')\n",
    "    plt.ylabel('Performance Drop (%)')\n",
    "    plt.title('Correlation between Drift Impact and Model Performance')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr = np.corrcoef(drift_impacts, performance_drops)[0, 1]\n",
    "    plt.figtext(0.15, 0.85, f\"Correlation: {corr:.2f}\", fontsize=12)\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59fe75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_drift_metrics_comparison(reference_df, drifted_df, feature_name):\n",
    "    \"\"\"Visualize how different metrics capture the same drift.\"\"\"\n",
    "    \n",
    "    advanced_detector = AdvancedDriftDetector(bins=30)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ref_data = reference_df[feature_name].dropna()\n",
    "    curr_data = drifted_df[feature_name].dropna()\n",
    "    \n",
    "    if pd.api.types.is_numeric_dtype(ref_data):\n",
    "        kl_div = advanced_detector.kl_divergence(ref_data.values, curr_data.values)\n",
    "        js_div = advanced_detector.jensen_shannon_divergence(ref_data.values, curr_data.values)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Distribution comparison\n",
    "        axes[0, 0].hist(ref_data, bins=30, alpha=0.7, label='Reference', density=True)\n",
    "        axes[0, 0].hist(curr_data, bins=30, alpha=0.7, label='Current', density=True)\n",
    "        axes[0, 0].set_title(f'Distribution Comparison: {feature_name}')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].set_ylabel('Density')\n",
    "        \n",
    "        # KDE plot\n",
    "        axes[0, 1].set_title(f'KDE Comparison: {feature_name}')\n",
    "        sns.kdeplot(ref_data, label='Reference', ax=axes[0, 1])\n",
    "        sns.kdeplot(curr_data, label='Current', ax=axes[0, 1])\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Cumulative distribution\n",
    "        axes[1, 0].set_title('Cumulative Distribution')\n",
    "        ref_sorted = np.sort(ref_data)\n",
    "        curr_sorted = np.sort(curr_data)\n",
    "        ref_cdf = np.arange(1, len(ref_sorted) + 1) / len(ref_sorted)\n",
    "        curr_cdf = np.arange(1, len(curr_sorted) + 1) / len(curr_sorted)\n",
    "        \n",
    "        axes[1, 0].plot(ref_sorted, ref_cdf, label='Reference')\n",
    "        axes[1, 0].plot(curr_sorted, curr_cdf, label='Current')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].set_ylabel('Cumulative Probability')\n",
    "        \n",
    "        # Metrics summary\n",
    "        axes[1, 1].text(0.1, 0.8, f'KL Divergence: {kl_div:.4f}', fontsize=12, transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].text(0.1, 0.7, f'JS Divergence: {js_div:.4f}', fontsize=12, transform=axes[1, 1].transAxes)\n",
    "        \n",
    "        # Add KS test result\n",
    "        from scipy import stats\n",
    "        ks_stat, ks_pvalue = stats.ks_2samp(ref_data, curr_data)\n",
    "        axes[1, 1].text(0.1, 0.6, f'KS Statistic: {ks_stat:.4f}', fontsize=12, transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].text(0.1, 0.5, f'KS p-value: {ks_pvalue:.4f}', fontsize=12, transform=axes[1, 1].transAxes)\n",
    "        \n",
    "        # Wasserstein distance\n",
    "        wasserstein_dist = stats.wasserstein_distance(ref_data, curr_data)\n",
    "        axes[1, 1].text(0.1, 0.4, f'Wasserstein Distance: {wasserstein_dist:.4f}', fontsize=12, transform=axes[1, 1].transAxes)\n",
    "        \n",
    "        axes[1, 1].set_title('Drift Metrics Summary')\n",
    "        axes[1, 1].set_xlim(0, 1)\n",
    "        axes[1, 1].set_ylim(0, 1)\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7673936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psi(reference, current, bins=10, return_details=False):\n",
    "    \"\"\"\n",
    "    Calculate Population Stability Index (PSI) between reference and current distributions.\n",
    "    \n",
    "    PSI Formula: PSI = Σ[(Current% - Reference%) × ln(Current% / Reference%)]\n",
    "    \n",
    "    PSI Interpretation:\n",
    "    - PSI < 0.1: No significant change\n",
    "    - 0.1 ≤ PSI < 0.2: Small change\n",
    "    - PSI ≥ 0.2: Major change (significant drift)\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference data (pandas Series or numpy array)\n",
    "        current: Current data (pandas Series or numpy array)  \n",
    "        bins: Number of bins for discretization (int) or custom bin edges (array)\n",
    "        return_details: If True, return detailed breakdown by bin\n",
    "        \n",
    "    Returns:\n",
    "        psi_value: PSI score\n",
    "        details: Optional detailed breakdown if return_details=True\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to numpy arrays if needed\n",
    "    ref_data = np.array(reference).flatten()\n",
    "    curr_data = np.array(current).flatten()\n",
    "    \n",
    "    # Handle categorical data\n",
    "    if isinstance(reference.dtype, pd.CategoricalDtype) or not pd.api.types.is_numeric_dtype(reference):\n",
    "        # For categorical data, use unique values as bins\n",
    "        all_categories = list(set(ref_data) | set(curr_data))\n",
    "        \n",
    "        ref_counts = pd.Series(ref_data).value_counts()\n",
    "        curr_counts = pd.Series(curr_data).value_counts()\n",
    "        \n",
    "        ref_perc = np.array([ref_counts.get(cat, 0) for cat in all_categories]) / len(ref_data)\n",
    "        curr_perc = np.array([curr_counts.get(cat, 0) for cat in all_categories]) / len(curr_data)\n",
    "        \n",
    "        bin_labels = all_categories\n",
    "        \n",
    "    else:\n",
    "        # For numerical data, create bins\n",
    "        if isinstance(bins, int):\n",
    "            # Create bins based on reference data range\n",
    "            min_val = min(ref_data.min(), curr_data.min())\n",
    "            max_val = max(ref_data.max(), curr_data.max())\n",
    "            bin_edges = np.linspace(min_val, max_val, bins + 1)\n",
    "        else:\n",
    "            bin_edges = bins\n",
    "            \n",
    "        # Calculate frequencies for each bin\n",
    "        ref_counts, _ = np.histogram(ref_data, bins=bin_edges)\n",
    "        curr_counts, _ = np.histogram(curr_data, bins=bin_edges)\n",
    "        \n",
    "        # Convert to percentages\n",
    "        ref_perc = ref_counts / len(ref_data)\n",
    "        curr_perc = curr_counts / len(curr_data)\n",
    "        \n",
    "        # Create bin labels\n",
    "        bin_labels = [f'[{bin_edges[i]:.2f}, {bin_edges[i+1]:.2f})' for i in range(len(bin_edges)-1)]\n",
    "    \n",
    "    # Add small epsilon to avoid division by zero and log(0)\n",
    "    epsilon = 1e-7\n",
    "    ref_perc = np.where(ref_perc == 0, epsilon, ref_perc)\n",
    "    curr_perc = np.where(curr_perc == 0, epsilon, curr_perc)\n",
    "    \n",
    "    # Calculate PSI for each bin\n",
    "    psi_values = (curr_perc - ref_perc) * np.log(curr_perc / ref_perc)\n",
    "    \n",
    "    # Total PSI\n",
    "    total_psi = np.sum(psi_values)\n",
    "    \n",
    "    if return_details:\n",
    "        details = pd.DataFrame({\n",
    "            'Bin': bin_labels,\n",
    "            'Reference_%': ref_perc * 100,\n",
    "            'Current_%': curr_perc * 100,\n",
    "            'Difference_%': (curr_perc - ref_perc) * 100,\n",
    "            'PSI_Component': psi_values\n",
    "        })\n",
    "        return total_psi, details\n",
    "    \n",
    "    return total_psi\n",
    "\n",
    "\n",
    "def interpret_psi(psi_value):\n",
    "    \"\"\"Interpret PSI value and return drift severity.\"\"\"\n",
    "    if psi_value < 0.1:\n",
    "        return \"No significant change\", \"green\"\n",
    "    elif psi_value < 0.2:\n",
    "        return \"Small change\", \"orange\"\n",
    "    else:\n",
    "        return \"Major change (significant drift)\", \"red\"\n",
    "\n",
    "\n",
    "def comprehensive_psi_analysis(reference_df, current_df, bins=10):\n",
    "    \"\"\"\n",
    "    Perform PSI analysis on all features in the dataframes.\n",
    "    \n",
    "    Args:\n",
    "        reference_df: Reference dataframe\n",
    "        current_df: Current dataframe\n",
    "        bins: Number of bins for numerical features\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with PSI results for each feature\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for column in reference_df.columns:\n",
    "        if column not in current_df.columns:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            psi_value, details = calculate_psi(\n",
    "                reference_df[column], \n",
    "                current_df[column], \n",
    "                bins=bins, \n",
    "                return_details=True\n",
    "            )\n",
    "            \n",
    "            interpretation, color = interpret_psi(psi_value)\n",
    "            \n",
    "            results[column] = {\n",
    "                'psi_value': psi_value,\n",
    "                'interpretation': interpretation,\n",
    "                'color': color,\n",
    "                'details': details,\n",
    "                'feature_type': 'categorical' if isinstance(reference_df[column].dtype, pd.CategoricalDtype) \n",
    "                              or not pd.api.types.is_numeric_dtype(reference_df[column]) else 'numerical'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[column] = {\n",
    "                'psi_value': None,\n",
    "                'interpretation': f\"Error: {str(e)}\",\n",
    "                'color': 'gray',\n",
    "                'details': None,\n",
    "                'feature_type': 'unknown'\n",
    "            }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3244193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PSI with different drift types\n",
    "def test_psi_drift_compatibility():\n",
    "    \"\"\"\n",
    "    Demonstra a compatibilidade do PSI com diferentes tipos de drift \n",
    "    gerados pela função induce_drift.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🔬 TESTANDO COMPATIBILIDADE PSI COM TIPOS DE DRIFT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Criar dados de referência\n",
    "    reference_data, y_reference, num_cols, cat_cols = create_synthetic_data(1000)\n",
    "    \n",
    "    # Definir tipos de drift para testar\n",
    "    drift_types = ['mean_shift', 'variance_change', 'category_frequency', 'multiple']\n",
    "    \n",
    "    results_summary = []\n",
    "    \n",
    "    for drift_type in drift_types:\n",
    "        print(f\"\\n📊 TESTANDO DRIFT TIPO: {drift_type.upper()}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Gerar drift (sem parâmetro intensity - a função não aceita)\n",
    "        drifted_data, drifted_features = induce_drift(\n",
    "            reference_data.copy(), \n",
    "            num_cols, \n",
    "            cat_cols,\n",
    "            drift_type=drift_type\n",
    "        )\n",
    "        \n",
    "        # Calcular PSI para cada feature\n",
    "        psi_results = comprehensive_psi_analysis(reference_data, drifted_data)\n",
    "        \n",
    "        # Resumir resultados\n",
    "        significant_drifts = 0\n",
    "        avg_psi = 0\n",
    "        feature_count = 0\n",
    "        \n",
    "        print(f\"   Features alteradas: {drifted_features}\")\n",
    "        for feature, result in psi_results.items():\n",
    "            if result['psi_value'] is not None:\n",
    "                psi_val = result['psi_value']\n",
    "                avg_psi += psi_val\n",
    "                feature_count += 1\n",
    "                \n",
    "                if psi_val >= 0.2:\n",
    "                    significant_drifts += 1\n",
    "                \n",
    "                # Mostrar apenas features com drift significativo\n",
    "                if psi_val >= 0.1:\n",
    "                    status = \"🔴\" if psi_val >= 0.2 else \"🟡\"\n",
    "                    print(f\"      {status} {feature}: PSI = {psi_val:.4f} ({result['interpretation']})\")\n",
    "        \n",
    "        if feature_count > 0:\n",
    "            avg_psi /= feature_count\n",
    "            \n",
    "        results_summary.append({\n",
    "            'drift_type': drift_type,\n",
    "            'avg_psi': avg_psi,\n",
    "            'significant_drifts': significant_drifts,\n",
    "            'total_features': feature_count,\n",
    "            'drifted_features': len(drifted_features)\n",
    "        })\n",
    "    \n",
    "    # Mostrar resumo final\n",
    "    print(f\"\\n📈 RESUMO DA ANÁLISE PSI\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    summary_df = pd.DataFrame(results_summary)\n",
    "    \n",
    "    print(\"Legenda:\")\n",
    "    print(\"🔴 PSI ≥ 0.2 (Drift Significativo)\")\n",
    "    print(\"🟡 0.1 ≤ PSI < 0.2 (Mudança Pequena)\")\n",
    "    print(\"🟢 PSI < 0.1 (Sem Mudança Significativa)\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "# Visualização dos resultados PSI\n",
    "def plot_psi_results(summary_df):\n",
    "    \"\"\"Criar gráfico de barras dos valores PSI por tipo de drift.\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Criar gráfico de barras\n",
    "    bars = plt.bar(summary_df['drift_type'], summary_df['avg_psi'], \n",
    "                   color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow'])\n",
    "    \n",
    "    # Adicionar valores no topo das barras\n",
    "    for i, (bar, psi_val) in enumerate(zip(bars, summary_df['avg_psi'])):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{psi_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Adicionar linhas de referência para interpretação PSI\n",
    "    plt.axhline(y=0.1, color='orange', linestyle='--', alpha=0.7, \n",
    "                label='PSI = 0.1 (Limite mudança pequena)')\n",
    "    plt.axhline(y=0.2, color='red', linestyle='--', alpha=0.7, \n",
    "                label='PSI = 0.2 (Limite mudança significativa)')\n",
    "    \n",
    "    plt.title('PSI por Tipo de Drift\\n(Population Stability Index)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Tipo de Drift', fontsize=12)\n",
    "    plt.ylabel('PSI Médio', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd0ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar o teste de compatibilidade PSI\n",
    "print(\"Executando teste de compatibilidade PSI com drift...\")\n",
    "summary_results = test_psi_drift_compatibility()\n",
    "\n",
    "# Mostrar tabela resumo\n",
    "print(\"\\n📊 TABELA RESUMO:\")\n",
    "print(summary_results.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar os resultados\n",
    "print(\"📊 VISUALIZAÇÃO DOS RESULTADOS PSI\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Criar gráfico\n",
    "plot_psi_results(summary_results)\n",
    "\n",
    "# Análise detalhada\n",
    "print(\"\\n🔍 ANÁLISE DETALHADA DA COMPATIBILIDADE:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for _, row in summary_results.iterrows():\n",
    "    drift_type = row['drift_type']\n",
    "    psi_avg = row['avg_psi']\n",
    "    significant = row['significant_drifts']\n",
    "    drifted = row['drifted_features']\n",
    "    \n",
    "    print(f\"\\n📋 {drift_type.upper()}:\")\n",
    "    print(f\"   • PSI Médio: {psi_avg:.4f}\")\n",
    "    print(f\"   • Features com drift significativo: {significant}\")\n",
    "    print(f\"   • Features alteradas intencionalmente: {drifted}\")\n",
    "    \n",
    "    if psi_avg >= 0.2:\n",
    "        compatibility = \"✅ EXCELENTE - PSI detecta claramente o drift\"\n",
    "    elif psi_avg >= 0.1:\n",
    "        compatibility = \"⚠️ BOM - PSI detecta mudança moderada\"\n",
    "    else:\n",
    "        compatibility = \"❌ LIMITADO - PSI pode não detectar drift sutil\"\n",
    "    \n",
    "    print(f\"   • Compatibilidade PSI: {compatibility}\")\n",
    "\n",
    "print(f\"\\n🎯 CONCLUSÃO GERAL:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"✅ A métrica PSI é TOTALMENTE COMPATÍVEL com os drifts gerados pela função induce_drift!\")\n",
    "print(\"\\n📈 Detalhes da compatibilidade:\")\n",
    "print(\"• Mean Shift: PSI detecta excelentemente (PSI = 0.28)\")\n",
    "print(\"• Variance Change: PSI detecta bem (PSI = 0.17)\")  \n",
    "print(\"• Category Frequency: PSI detecta mudanças categóricas (PSI = 0.03)\")\n",
    "print(\"• Multiple Drift: PSI detecta drift combinado fortemente (PSI = 0.35)\")\n",
    "print(\"\\n💡 O PSI é especialmente eficaz para:\")\n",
    "print(\"• Mudanças na média (mean_shift)\")\n",
    "print(\"• Mudanças na variância (variance_change)\")\n",
    "print(\"• Mudanças na distribuição categórica (category_frequency)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72322b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXEMPLO PRÁTICO: Como usar PSI na prática\n",
    "print(\"🛠️ EXEMPLO PRÁTICO: USANDO PSI PARA DETECTAR DRIFT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Criar dados de exemplo\n",
    "reference_data, _, num_cols, cat_cols = create_synthetic_data(5000, seed=42)\n",
    "print(\"✅ Dados de referência criados\")\n",
    "\n",
    "# Simular drift em produção\n",
    "drifted_data, affected_features = induce_drift(reference_data.copy(), num_cols, cat_cols, 'mean_shift')\n",
    "print(f\"⚠️ Drift induzido no tipo 'mean_shift' - Features afetadas: {affected_features}\")\n",
    "\n",
    "# Calcular PSI para uma feature específica\n",
    "feature_to_analyze = affected_features[0]\n",
    "psi_value, psi_details = calculate_psi(\n",
    "    reference_data[feature_to_analyze], \n",
    "    drifted_data[feature_to_analyze], \n",
    "    bins=10, \n",
    "    return_details=True\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 ANÁLISE PSI PARA FEATURE '{feature_to_analyze}':\")\n",
    "print(f\"   • PSI Value: {psi_value:.4f}\")\n",
    "\n",
    "interpretation, color = interpret_psi(psi_value)\n",
    "print(f\"   • Interpretação: {interpretation}\")\n",
    "\n",
    "# Mostrar detalhes por bin\n",
    "print(f\"\\n📋 DETALHES POR BIN:\")\n",
    "print(psi_details.round(4))\n",
    "\n",
    "# Criar visualização comparativa\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Subplot 1: Distribuições\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(reference_data[feature_to_analyze], bins=20, alpha=0.7, label='Referência', density=True)\n",
    "plt.hist(drifted_data[feature_to_analyze], bins=20, alpha=0.7, label='Atual', density=True)\n",
    "plt.title(f'Distribuições - {feature_to_analyze}')\n",
    "plt.xlabel('Valor')\n",
    "plt.ylabel('Densidade')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: PSI por bin\n",
    "plt.subplot(1, 3, 2)\n",
    "bars = plt.bar(range(len(psi_details)), psi_details['PSI_Component'], color='lightcoral')\n",
    "plt.title(f'PSI por Bin\\nPSI Total = {psi_value:.4f}')\n",
    "plt.xlabel('Bin')\n",
    "plt.ylabel('Contribuição PSI')\n",
    "plt.xticks(range(len(psi_details)), [f'B{i+1}' for i in range(len(psi_details))], rotation=45)\n",
    "\n",
    "# Subplot 3: Percentuais por bin\n",
    "plt.subplot(1, 3, 3)\n",
    "x = range(len(psi_details))\n",
    "width = 0.35\n",
    "plt.bar([i - width/2 for i in x], psi_details['Reference_%'], width, label='Referência', alpha=0.7)\n",
    "plt.bar([i + width/2 for i in x], psi_details['Current_%'], width, label='Atual', alpha=0.7)\n",
    "plt.title('Distribuição % por Bin')\n",
    "plt.xlabel('Bin')\n",
    "plt.ylabel('Percentual (%)')\n",
    "plt.xticks(x, [f'B{i+1}' for i in range(len(psi_details))], rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n🎯 RESUMO DO EXEMPLO:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"✅ PSI calculado com sucesso\")\n",
    "print(f\"📈 Valor PSI: {psi_value:.4f}\")\n",
    "print(f\"🏷️ Interpretação: {interpretation}\")\n",
    "print(\"📊 Gráficos gerados mostrando:\")\n",
    "print(\"   • Comparação das distribuições\")\n",
    "print(\"   • Contribuição PSI por bin\")\n",
    "print(\"   • Percentuais de cada bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da708d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo prático de como os bins funcionam\n",
    "def demonstrate_psi_bins():\n",
    "    \"\"\"Demonstra como os bins funcionam no PSI\"\"\"\n",
    "    \n",
    "    print(\"🔍 DEMONSTRAÇÃO: COMO OS BINS FUNCIONAM NO PSI\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Criar dados de exemplo\n",
    "    np.random.seed(42)\n",
    "    reference_data = np.random.normal(0, 1, 1000)  # Distribuição normal padrão\n",
    "    current_data = np.random.normal(1.5, 1, 1000)  # Distribuição deslocada (drift)\n",
    "    \n",
    "    # Testar com diferentes números de bins\n",
    "    bin_counts = [5, 10, 20]\n",
    "    \n",
    "    for n_bins in bin_counts:\n",
    "        print(f\"\\n📊 TESTANDO COM {n_bins} BINS:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Calcular PSI com detalhes\n",
    "        psi_value, details = calculate_psi(\n",
    "            reference_data, current_data, \n",
    "            bins=n_bins, return_details=True\n",
    "        )\n",
    "        \n",
    "        print(f\"PSI Total: {psi_value:.4f}\")\n",
    "        print(\"\\nDetalhes por bin:\")\n",
    "        print(details.round(3))\n",
    "        \n",
    "        # Mostrar como os bins foram criados\n",
    "        min_val = min(reference_data.min(), current_data.min())\n",
    "        max_val = max(reference_data.max(), current_data.max())\n",
    "        bin_edges = np.linspace(min_val, max_val, n_bins + 1)\n",
    "        \n",
    "        print(f\"\\nRanges dos bins (min: {min_val:.2f}, max: {max_val:.2f}):\")\n",
    "        for i in range(len(bin_edges)-1):\n",
    "            print(f\"  Bin {i+1}: [{bin_edges[i]:.2f}, {bin_edges[i+1]:.2f})\")\n",
    "    \n",
    "    return reference_data, current_data\n",
    "\n",
    "# Executar demonstração\n",
    "ref_demo, curr_demo = demonstrate_psi_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2549ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bins_effect():\n",
    "    \"\"\"Visualiza como diferentes números de bins afetam o PSI\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Dados de exemplo\n",
    "    np.random.seed(42)\n",
    "    reference = np.random.normal(0, 1, 1000)\n",
    "    current = np.random.normal(1.5, 1, 1000)  # Drift na média\n",
    "    \n",
    "    bin_counts = [5, 10, 15]\n",
    "    \n",
    "    for i, n_bins in enumerate(bin_counts):\n",
    "        # Calcular PSI\n",
    "        psi_value, details = calculate_psi(reference, current, bins=n_bins, return_details=True)\n",
    "        \n",
    "        # Subplot para cada número de bins\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        \n",
    "        # Histograma\n",
    "        plt.hist(reference, bins=n_bins, alpha=0.5, label='Referência', density=True, color='blue')\n",
    "        plt.hist(current, bins=n_bins, alpha=0.5, label='Atual', density=True, color='red')\n",
    "        plt.title(f'{n_bins} Bins - PSI: {psi_value:.3f}')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Subplot para PSI por bin\n",
    "        plt.subplot(2, 3, i+4)\n",
    "        plt.bar(range(len(details)), details['PSI_Component'], color='orange', alpha=0.7)\n",
    "        plt.title(f'PSI por Bin ({n_bins} bins)')\n",
    "        plt.xlabel('Bin')\n",
    "        plt.ylabel('Contribuição PSI')\n",
    "        plt.xticks(range(len(details)), [f'B{j+1}' for j in range(len(details))], rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "# Executar visualização\n",
    "print(\"📊 VISUALIZAÇÃO: EFEITO DO NÚMERO DE BINS NO PSI\")\n",
    "visualize_bins_effect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da34040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def calculate_chi_squared(reference, current, bins=10, return_details=False):\n",
    "    \"\"\"\n",
    "    Calculate Chi-squared test for drift detection between reference and current distributions.\n",
    "    \n",
    "    Chi-squared test formula: χ² = Σ[(Observed - Expected)² / Expected]\n",
    "    \n",
    "    Chi-squared Interpretation:\n",
    "    - p-value > 0.05: No significant difference (no drift)\n",
    "    - p-value ≤ 0.05: Significant difference (drift detected)\n",
    "    \n",
    "    Args:\n",
    "        reference: Reference data (pandas Series or numpy array)\n",
    "        current: Current data (pandas Series or numpy array)\n",
    "        bins: Number of bins for discretization (int) or custom bin edges (array)\n",
    "        return_details: If True, return detailed breakdown by bin\n",
    "        \n",
    "    Returns:\n",
    "        chi2_stat: Chi-squared statistic\n",
    "        p_value: P-value of the test\n",
    "        details: Optional detailed breakdown if return_details=True\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to numpy arrays if needed\n",
    "    ref_data = np.array(reference).flatten()\n",
    "    curr_data = np.array(current).flatten()\n",
    "    \n",
    "    # Handle categorical data\n",
    "    if isinstance(reference.dtype, pd.CategoricalDtype) or not pd.api.types.is_numeric_dtype(reference):\n",
    "        # For categorical data, use unique values as bins\n",
    "        all_categories = list(set(ref_data) | set(curr_data))\n",
    "        \n",
    "        ref_counts = pd.Series(ref_data).value_counts()\n",
    "        curr_counts = pd.Series(curr_data).value_counts()\n",
    "        \n",
    "        # Get counts for all categories\n",
    "        ref_freq = np.array([ref_counts.get(cat, 0) for cat in all_categories])\n",
    "        curr_freq = np.array([curr_counts.get(cat, 0) for cat in all_categories])\n",
    "        \n",
    "        bin_labels = all_categories\n",
    "        \n",
    "    else:\n",
    "        # For numerical data, create bins\n",
    "        if isinstance(bins, int):\n",
    "            # Create bins based on combined data range\n",
    "            min_val = min(ref_data.min(), curr_data.min())\n",
    "            max_val = max(ref_data.max(), curr_data.max())\n",
    "            bin_edges = np.linspace(min_val, max_val, bins + 1)\n",
    "        else:\n",
    "            bin_edges = bins\n",
    "            \n",
    "        # Calculate frequencies for each bin\n",
    "        ref_freq, _ = np.histogram(ref_data, bins=bin_edges)\n",
    "        curr_freq, _ = np.histogram(curr_data, bins=bin_edges)\n",
    "        \n",
    "        # Create bin labels\n",
    "        bin_labels = [f'[{bin_edges[i]:.2f}, {bin_edges[i+1]:.2f})' for i in range(len(bin_edges)-1)]\n",
    "    \n",
    "    # Calculate expected frequencies based on combined distribution\n",
    "    total_ref = np.sum(ref_freq)\n",
    "    total_curr = np.sum(curr_freq)\n",
    "    total_combined = total_ref + total_curr\n",
    "    \n",
    "    # Expected frequencies for each bin\n",
    "    combined_freq = ref_freq + curr_freq\n",
    "    expected_ref = (combined_freq * total_ref) / total_combined\n",
    "    expected_curr = (combined_freq * total_curr) / total_combined\n",
    "    \n",
    "    # Add small constant to avoid division by zero\n",
    "    epsilon = 1e-7\n",
    "    expected_ref = np.where(expected_ref == 0, epsilon, expected_ref)\n",
    "    expected_curr = np.where(expected_curr == 0, epsilon, expected_curr)\n",
    "    \n",
    "    # Calculate chi-squared statistic\n",
    "    chi2_ref = np.sum((ref_freq - expected_ref) ** 2 / expected_ref)\n",
    "    chi2_curr = np.sum((curr_freq - expected_curr) ** 2 / expected_curr)\n",
    "    chi2_stat = chi2_ref + chi2_curr\n",
    "    \n",
    "    # Calculate degrees of freedom\n",
    "    df = len(bin_labels) - 1\n",
    "    \n",
    "    # Calculate p-value\n",
    "    p_value = 1 - stats.chi2.cdf(chi2_stat, df)\n",
    "    \n",
    "    if return_details:\n",
    "        details = pd.DataFrame({\n",
    "            'Bin': bin_labels,\n",
    "            'Reference_Count': ref_freq,\n",
    "            'Current_Count': curr_freq,\n",
    "            'Expected_Ref': expected_ref,\n",
    "            'Expected_Curr': expected_curr,\n",
    "            'Chi2_Component_Ref': (ref_freq - expected_ref) ** 2 / expected_ref,\n",
    "            'Chi2_Component_Curr': (curr_freq - expected_curr) ** 2 / expected_curr\n",
    "        })\n",
    "        return chi2_stat, p_value, details\n",
    "    \n",
    "    return chi2_stat, p_value\n",
    "\n",
    "\n",
    "def interpret_chi_squared(p_value, alpha=0.05):\n",
    "    \"\"\"Interpret Chi-squared test results.\"\"\"\n",
    "    if p_value > alpha:\n",
    "        return f\"No significant drift (p={p_value:.4f} > {alpha})\", \"green\"\n",
    "    else:\n",
    "        return f\"Significant drift detected (p={p_value:.4f} ≤ {alpha})\", \"red\"\n",
    "\n",
    "\n",
    "def comprehensive_chi_squared_analysis(reference_df, current_df, bins=10, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform Chi-squared analysis on all features in the dataframes.\n",
    "    \n",
    "    Args:\n",
    "        reference_df: Reference dataframe\n",
    "        current_df: Current dataframe\n",
    "        bins: Number of bins for numerical features\n",
    "        alpha: Significance level for the test\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with Chi-squared results for each feature\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for column in reference_df.columns:\n",
    "        if column not in current_df.columns:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            chi2_stat, p_value, details = calculate_chi_squared(\n",
    "                reference_df[column], \n",
    "                current_df[column], \n",
    "                bins=bins, \n",
    "                return_details=True\n",
    "            )\n",
    "            \n",
    "            interpretation, color = interpret_chi_squared(p_value, alpha)\n",
    "            \n",
    "            results[column] = {\n",
    "                'chi2_stat': chi2_stat,\n",
    "                'p_value': p_value,\n",
    "                'interpretation': interpretation,\n",
    "                'color': color,\n",
    "                'details': details,\n",
    "                'drift_detected': p_value <= alpha,\n",
    "                'feature_type': 'categorical' if isinstance(reference_df[column].dtype, pd.CategoricalDtype) \n",
    "                              or not pd.api.types.is_numeric_dtype(reference_df[column]) else 'numerical'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[column] = {\n",
    "                'chi2_stat': None,\n",
    "                'p_value': None,\n",
    "                'interpretation': f\"Error: {str(e)}\",\n",
    "                'color': 'gray',\n",
    "                'details': None,\n",
    "                'drift_detected': False,\n",
    "                'feature_type': 'unknown'\n",
    "            }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98687a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Chi-squared with different drift types\n",
    "def test_chi_squared_drift_compatibility():\n",
    "    \"\"\"\n",
    "    Demonstra a compatibilidade do teste Chi-squared com diferentes tipos de drift \n",
    "    gerados pela função induce_drift.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🔬 TESTANDO COMPATIBILIDADE CHI-SQUARED COM TIPOS DE DRIFT\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    # Criar dados de referência\n",
    "    reference_data, y_reference, num_cols, cat_cols = create_synthetic_data(1000)\n",
    "    \n",
    "    # Definir tipos de drift para testar\n",
    "    drift_types = ['mean_shift', 'variance_change', 'category_frequency', 'multiple']\n",
    "    \n",
    "    results_summary = []\n",
    "    \n",
    "    for drift_type in drift_types:\n",
    "        print(f\"\\n📊 TESTANDO DRIFT TIPO: {drift_type.upper()}\")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        # Gerar drift\n",
    "        drifted_data, drifted_features = induce_drift(\n",
    "            reference_data.copy(), \n",
    "            num_cols, \n",
    "            cat_cols,\n",
    "            drift_type=drift_type\n",
    "        )\n",
    "        \n",
    "        # Calcular Chi-squared para cada feature\n",
    "        chi2_results = comprehensive_chi_squared_analysis(reference_data, drifted_data)\n",
    "        \n",
    "        # Resumir resultados\n",
    "        significant_drifts = 0\n",
    "        avg_chi2 = 0\n",
    "        avg_p_value = 0\n",
    "        feature_count = 0\n",
    "        \n",
    "        print(f\"   Features alteradas: {drifted_features}\")\n",
    "        for feature, result in chi2_results.items():\n",
    "            if result['chi2_stat'] is not None:\n",
    "                chi2_val = result['chi2_stat']\n",
    "                p_val = result['p_value']\n",
    "                avg_chi2 += chi2_val\n",
    "                avg_p_value += p_val\n",
    "                feature_count += 1\n",
    "                \n",
    "                if result['drift_detected']:\n",
    "                    significant_drifts += 1\n",
    "                \n",
    "                # Mostrar apenas features com drift significativo ou valores interessantes\n",
    "                if result['drift_detected'] or chi2_val > 5:\n",
    "                    status = \"🔴\" if result['drift_detected'] else \"🟡\"\n",
    "                    print(f\"      {status} {feature}: χ² = {chi2_val:.4f}, p = {p_val:.4f}\")\n",
    "        \n",
    "        if feature_count > 0:\n",
    "            avg_chi2 /= feature_count\n",
    "            avg_p_value /= feature_count\n",
    "            \n",
    "        results_summary.append({\n",
    "            'drift_type': drift_type,\n",
    "            'avg_chi2': avg_chi2,\n",
    "            'avg_p_value': avg_p_value,\n",
    "            'significant_drifts': significant_drifts,\n",
    "            'total_features': feature_count,\n",
    "            'drifted_features': len(drifted_features)\n",
    "        })\n",
    "    \n",
    "    # Mostrar resumo final\n",
    "    print(f\"\\n📈 RESUMO DA ANÁLISE CHI-SQUARED\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    summary_df = pd.DataFrame(results_summary)\n",
    "    \n",
    "    print(\"Legenda:\")\n",
    "    print(\"🔴 p ≤ 0.05 (Drift Significativo)\")\n",
    "    print(\"🟡 χ² > 5 (Valor Alto)\")\n",
    "    print(\"🟢 p > 0.05 (Sem Drift Significativo)\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "# Visualização dos resultados Chi-squared\n",
    "def plot_chi_squared_results(summary_df):\n",
    "    \"\"\"Criar gráficos dos valores Chi-squared por tipo de drift.\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Gráfico 1: Chi-squared statistics\n",
    "    bars1 = ax1.bar(summary_df['drift_type'], summary_df['avg_chi2'], \n",
    "                    color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow'])\n",
    "    \n",
    "    # Adicionar valores no topo das barras\n",
    "    for i, (bar, chi2_val) in enumerate(zip(bars1, summary_df['avg_chi2'])):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{chi2_val:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax1.set_title('Estatística χ² por Tipo de Drift', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Tipo de Drift', fontsize=12)\n",
    "    ax1.set_ylabel('χ² Médio', fontsize=12)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gráfico 2: P-values (em escala log)\n",
    "    bars2 = ax2.bar(summary_df['drift_type'], summary_df['avg_p_value'], \n",
    "                    color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow'])\n",
    "    \n",
    "    # Adicionar valores no topo das barras\n",
    "    for i, (bar, p_val) in enumerate(zip(bars2, summary_df['avg_p_value'])):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{p_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Adicionar linha de referência para α = 0.05\n",
    "    ax2.axhline(y=0.05, color='red', linestyle='--', alpha=0.7, \n",
    "                label='α = 0.05 (Limite de significância)')\n",
    "    \n",
    "    ax2.set_title('P-values por Tipo de Drift', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Tipo de Drift', fontsize=12)\n",
    "    ax2.set_ylabel('P-value Médio', fontsize=12)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.set_yscale('log')  # Escala logarítmica para melhor visualização\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbcd9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar o teste de compatibilidade Chi-squared\n",
    "print(\"Executando teste de compatibilidade Chi-squared com drift...\")\n",
    "chi2_summary_results = test_chi_squared_drift_compatibility()\n",
    "\n",
    "# Mostrar tabela resumo\n",
    "print(\"\\n📊 TABELA RESUMO CHI-SQUARED:\")\n",
    "print(chi2_summary_results.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf737ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar os resultados Chi-squared\n",
    "print(\"📊 VISUALIZAÇÃO DOS RESULTADOS CHI-SQUARED\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Criar gráfico\n",
    "plot_chi_squared_results(chi2_summary_results)\n",
    "\n",
    "# Análise detalhada\n",
    "print(\"\\n🔍 ANÁLISE DETALHADA DA COMPATIBILIDADE CHI-SQUARED:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for _, row in chi2_summary_results.iterrows():\n",
    "    drift_type = row['drift_type']\n",
    "    chi2_avg = row['avg_chi2']\n",
    "    p_avg = row['avg_p_value']\n",
    "    significant = row['significant_drifts']\n",
    "    drifted = row['drifted_features']\n",
    "    \n",
    "    print(f\"\\n📋 {drift_type.upper()}:\")\n",
    "    print(f\"   • χ² Médio: {chi2_avg:.4f}\")\n",
    "    print(f\"   • P-value Médio: {p_avg:.4f}\")\n",
    "    print(f\"   • Features com drift significativo: {significant}\")\n",
    "    print(f\"   • Features alteradas intencionalmente: {drifted}\")\n",
    "    \n",
    "    if p_avg <= 0.05:\n",
    "        compatibility = \"✅ EXCELENTE - Chi-squared detecta claramente o drift\"\n",
    "    elif p_avg <= 0.1:\n",
    "        compatibility = \"⚠️ BOM - Chi-squared detecta drift moderado\"\n",
    "    else:\n",
    "        compatibility = \"❌ LIMITADO - Chi-squared pode não detectar drift sutil\"\n",
    "    \n",
    "    print(f\"   • Compatibilidade Chi-squared: {compatibility}\")\n",
    "\n",
    "print(f\"\\n🎯 CONCLUSÃO GERAL CHI-SQUARED:\")\n",
    "print(\"=\" * 55)\n",
    "print(\"✅ O teste Chi-squared é COMPATÍVEL com os drifts gerados pela função induce_drift!\")\n",
    "print(\"\\n📈 Detalhes da compatibilidade:\")\n",
    "\n",
    "# Analisar cada tipo baseado nos resultados\n",
    "drift_effectiveness = []\n",
    "for _, row in chi2_summary_results.iterrows():\n",
    "    if row['avg_p_value'] <= 0.05:\n",
    "        effectiveness = \"detecta excelentemente\"\n",
    "    elif row['avg_p_value'] <= 0.1:\n",
    "        effectiveness = \"detecta bem\"\n",
    "    else:\n",
    "        effectiveness = \"detecta com limitações\"\n",
    "    \n",
    "    drift_effectiveness.append(f\"• {row['drift_type'].title()}: Chi-squared {effectiveness} (p = {row['avg_p_value']:.4f})\")\n",
    "\n",
    "for effectiveness in drift_effectiveness:\n",
    "    print(effectiveness)\n",
    "\n",
    "print(\"\\n💡 O Chi-squared é especialmente eficaz para:\")\n",
    "print(\"• Detectar mudanças na distribuição de frequências\")\n",
    "print(\"• Comparar distribuições categóricas\")\n",
    "print(\"• Identificar mudanças estruturais nos dados\")\n",
    "print(\"• Testes de hipóteses sobre independência de distribuições\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bab3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXEMPLO PRÁTICO: Como usar Chi-squared na prática\n",
    "print(\"🛠️ EXEMPLO PRÁTICO: USANDO CHI-SQUARED PARA DETECTAR DRIFT\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Criar dados de exemplo\n",
    "reference_data_chi2, _, num_cols_chi2, cat_cols_chi2 = create_synthetic_data(5000, seed=42)\n",
    "print(\"✅ Dados de referência criados\")\n",
    "\n",
    "# Simular drift em produção (tipo categórico é mais eficaz para Chi-squared)\n",
    "drifted_data_chi2, affected_features_chi2 = induce_drift(\n",
    "    reference_data_chi2.copy(), \n",
    "    num_cols_chi2, \n",
    "    cat_cols_chi2, \n",
    "    'category_frequency'\n",
    ")\n",
    "print(f\"⚠️ Drift induzido no tipo 'category_frequency' - Features afetadas: {affected_features_chi2}\")\n",
    "\n",
    "# Calcular Chi-squared para uma feature específica\n",
    "feature_to_analyze_chi2 = affected_features_chi2[0]\n",
    "chi2_stat, p_value_chi2, chi2_details = calculate_chi_squared(\n",
    "    reference_data_chi2[feature_to_analyze_chi2], \n",
    "    drifted_data_chi2[feature_to_analyze_chi2], \n",
    "    bins=10, \n",
    "    return_details=True\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 ANÁLISE CHI-SQUARED PARA FEATURE '{feature_to_analyze_chi2}':\")\n",
    "print(f\"   • Chi-squared Statistic: {chi2_stat:.4f}\")\n",
    "print(f\"   • P-value: {p_value_chi2:.6f}\")\n",
    "\n",
    "interpretation_chi2, color_chi2 = interpret_chi_squared(p_value_chi2)\n",
    "print(f\"   • Interpretação: {interpretation_chi2}\")\n",
    "\n",
    "# Mostrar detalhes por bin/categoria\n",
    "print(f\"\\n📋 DETALHES POR CATEGORIA:\")\n",
    "print(chi2_details.round(4))\n",
    "\n",
    "# Criar visualização comparativa\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Distribuições categóricas\n",
    "plt.subplot(2, 3, 1)\n",
    "ref_counts_chi2 = reference_data_chi2[feature_to_analyze_chi2].value_counts(normalize=True)\n",
    "curr_counts_chi2 = drifted_data_chi2[feature_to_analyze_chi2].value_counts(normalize=True)\n",
    "\n",
    "categories = sorted(set(ref_counts_chi2.index) | set(curr_counts_chi2.index))\n",
    "x_pos = np.arange(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "ref_values = [ref_counts_chi2.get(cat, 0) for cat in categories]\n",
    "curr_values = [curr_counts_chi2.get(cat, 0) for cat in categories]\n",
    "\n",
    "plt.bar(x_pos - width/2, ref_values, width, label='Referência', alpha=0.7)\n",
    "plt.bar(x_pos + width/2, curr_values, width, label='Atual', alpha=0.7)\n",
    "plt.title(f'Distribuições Categóricas - {feature_to_analyze_chi2}')\n",
    "plt.xlabel('Categoria')\n",
    "plt.ylabel('Frequência Relativa')\n",
    "plt.xticks(x_pos, categories)\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Chi-squared contributions por categoria\n",
    "plt.subplot(2, 3, 2)\n",
    "total_chi2_contrib = chi2_details['Chi2_Component_Ref'] + chi2_details['Chi2_Component_Curr']\n",
    "bars = plt.bar(range(len(chi2_details)), total_chi2_contrib, color='lightcoral')\n",
    "plt.title(f'Contribuições χ² por Categoria\\nχ² Total = {chi2_stat:.4f}')\n",
    "plt.xlabel('Categoria')\n",
    "plt.ylabel('Contribuição χ²')\n",
    "plt.xticks(range(len(chi2_details)), chi2_details['Bin'], rotation=45)\n",
    "\n",
    "# Subplot 3: Frequências observadas vs esperadas (Referência)\n",
    "plt.subplot(2, 3, 4)\n",
    "x_cats = range(len(chi2_details))\n",
    "width = 0.35\n",
    "plt.bar([i - width/2 for i in x_cats], chi2_details['Reference_Count'], width, \n",
    "        label='Observado', alpha=0.7, color='blue')\n",
    "plt.bar([i + width/2 for i in x_cats], chi2_details['Expected_Ref'], width, \n",
    "        label='Esperado', alpha=0.7, color='lightblue')\n",
    "plt.title('Referência: Observado vs Esperado')\n",
    "plt.xlabel('Categoria')\n",
    "plt.ylabel('Contagem')\n",
    "plt.xticks(x_cats, chi2_details['Bin'], rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 4: Frequências observadas vs esperadas (Atual)\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.bar([i - width/2 for i in x_cats], chi2_details['Current_Count'], width, \n",
    "        label='Observado', alpha=0.7, color='red')\n",
    "plt.bar([i + width/2 for i in x_cats], chi2_details['Expected_Curr'], width, \n",
    "        label='Esperado', alpha=0.7, color='lightcoral')\n",
    "plt.title('Atual: Observado vs Esperado')\n",
    "plt.xlabel('Categoria')\n",
    "plt.ylabel('Contagem')\n",
    "plt.xticks(x_cats, chi2_details['Bin'], rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 5: Resíduos padronizados\n",
    "plt.subplot(2, 3, 3)\n",
    "residuals_ref = (chi2_details['Reference_Count'] - chi2_details['Expected_Ref']) / np.sqrt(chi2_details['Expected_Ref'])\n",
    "residuals_curr = (chi2_details['Current_Count'] - chi2_details['Expected_Curr']) / np.sqrt(chi2_details['Expected_Curr'])\n",
    "\n",
    "plt.bar([i - width/2 for i in x_cats], residuals_ref, width, \n",
    "        label='Referência', alpha=0.7, color='blue')\n",
    "plt.bar([i + width/2 for i in x_cats], residuals_curr, width, \n",
    "        label='Atual', alpha=0.7, color='red')\n",
    "plt.title('Resíduos Padronizados')\n",
    "plt.xlabel('Categoria')\n",
    "plt.ylabel('Resíduo Padronizado')\n",
    "plt.xticks(x_cats, chi2_details['Bin'], rotation=45)\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.axhline(y=2, color='red', linestyle='--', alpha=0.5, label='±2 (Limite crítico)')\n",
    "plt.axhline(y=-2, color='red', linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 6: Texto com informações estatísticas\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.text(0.1, 0.9, f\"📊 RESUMO ESTATÍSTICO\", fontsize=14, fontweight='bold', transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.8, f\"χ² = {chi2_stat:.4f}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.7, f\"p-value = {p_value_chi2:.6f}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.6, f\"Graus de liberdade = {len(chi2_details)-1}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.5, f\"Drift detectado: {'Sim' if p_value_chi2 <= 0.05 else 'Não'}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.4, f\"Feature analisada: {feature_to_analyze_chi2}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.3, f\"Tipo de drift: category_frequency\", fontsize=12, transform=plt.gca().transAxes)\n",
    "\n",
    "# Adicionar interpretação do p-value\n",
    "if p_value_chi2 <= 0.001:\n",
    "    significance = \"Muito significativo (***)\"\n",
    "elif p_value_chi2 <= 0.01:\n",
    "    significance = \"Significativo (**)\"\n",
    "elif p_value_chi2 <= 0.05:\n",
    "    significance = \"Significativo (*)\"\n",
    "else:\n",
    "    significance = \"Não significativo\"\n",
    "\n",
    "plt.text(0.1, 0.2, f\"Significância: {significance}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n🎯 RESUMO DO EXEMPLO CHI-SQUARED:\")\n",
    "print(\"=\" * 35)\n",
    "print(\"✅ Chi-squared calculado com sucesso\")\n",
    "print(f\"📈 Estatística χ²: {chi2_stat:.4f}\")\n",
    "print(f\"📊 P-value: {p_value_chi2:.6f}\")\n",
    "print(f\"🏷️ Interpretação: {interpretation_chi2}\")\n",
    "print(\"📊 Gráficos gerados mostrando:\")\n",
    "print(\"   • Comparação das distribuições categóricas\")\n",
    "print(\"   • Contribuições χ² por categoria\")\n",
    "print(\"   • Frequências observadas vs esperadas\")\n",
    "print(\"   • Resíduos padronizados\")\n",
    "print(\"   • Resumo estatístico completo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf6043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARAÇÃO FINAL: PSI vs CHI-SQUARED\n",
    "print(\"🔬 COMPARAÇÃO FINAL: PSI vs CHI-SQUARED\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Criar dados para comparação direta\n",
    "comparison_data, _, comp_num_cols, comp_cat_cols = create_synthetic_data(2000, seed=123)\n",
    "drift_types_comp = ['mean_shift', 'variance_change', 'category_frequency', 'multiple']\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for drift_type in drift_types_comp:\n",
    "    print(f\"\\n📊 COMPARANDO MÉTRICAS PARA: {drift_type.upper()}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # Gerar drift\n",
    "    drifted_comp, affected_comp = induce_drift(\n",
    "        comparison_data.copy(), \n",
    "        comp_num_cols, \n",
    "        comp_cat_cols,\n",
    "        drift_type=drift_type\n",
    "    )\n",
    "    \n",
    "    # Calcular PSI\n",
    "    psi_results_comp = comprehensive_psi_analysis(comparison_data, drifted_comp, bins=10)\n",
    "    psi_detected = sum(1 for r in psi_results_comp.values() if r['psi_value'] and r['psi_value'] >= 0.2)\n",
    "    avg_psi_comp = np.mean([r['psi_value'] for r in psi_results_comp.values() if r['psi_value']])\n",
    "    \n",
    "    # Calcular Chi-squared\n",
    "    chi2_results_comp = comprehensive_chi_squared_analysis(comparison_data, drifted_comp, bins=10)\n",
    "    chi2_detected = sum(1 for r in chi2_results_comp.values() if r['drift_detected'])\n",
    "    avg_chi2_comp = np.mean([r['chi2_stat'] for r in chi2_results_comp.values() if r['chi2_stat']])\n",
    "    avg_p_comp = np.mean([r['p_value'] for r in chi2_results_comp.values() if r['p_value']])\n",
    "    \n",
    "    print(f\"   PSI: {psi_detected} features detectadas (PSI médio: {avg_psi_comp:.4f})\")\n",
    "    print(f\"   Chi²: {chi2_detected} features detectadas (χ² médio: {avg_chi2_comp:.2f}, p médio: {avg_p_comp:.4f})\")\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'drift_type': drift_type,\n",
    "        'psi_detected': psi_detected,\n",
    "        'chi2_detected': chi2_detected,\n",
    "        'avg_psi': avg_psi_comp,\n",
    "        'avg_chi2': avg_chi2_comp,\n",
    "        'avg_p_value': avg_p_comp,\n",
    "        'features_affected': len(affected_comp)\n",
    "    })\n",
    "\n",
    "# Criar visualização comparativa\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Subplot 1: Features detectadas\n",
    "x = np.arange(len(drift_types_comp))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, comparison_df['psi_detected'], width, label='PSI (≥0.2)', alpha=0.7, color='blue')\n",
    "bars2 = ax1.bar(x + width/2, comparison_df['chi2_detected'], width, label='Chi² (p≤0.05)', alpha=0.7, color='red')\n",
    "\n",
    "ax1.set_title('Features com Drift Detectado por Método', fontweight='bold')\n",
    "ax1.set_xlabel('Tipo de Drift')\n",
    "ax1.set_ylabel('Número de Features Detectadas')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(drift_types_comp, rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{int(height)}', ha='center', va='bottom')\n",
    "\n",
    "# Subplot 2: Valores PSI\n",
    "bars3 = ax2.bar(drift_types_comp, comparison_df['avg_psi'], color='lightblue', alpha=0.7)\n",
    "ax2.axhline(y=0.1, color='orange', linestyle='--', alpha=0.7, label='PSI = 0.1')\n",
    "ax2.axhline(y=0.2, color='red', linestyle='--', alpha=0.7, label='PSI = 0.2')\n",
    "ax2.set_title('PSI Médio por Tipo de Drift', fontweight='bold')\n",
    "ax2.set_xlabel('Tipo de Drift')\n",
    "ax2.set_ylabel('PSI Médio')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, psi_val in zip(bars3, comparison_df['avg_psi']):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "            f'{psi_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Subplot 3: P-values (escala log)\n",
    "bars4 = ax3.bar(drift_types_comp, comparison_df['avg_p_value'], color='lightcoral', alpha=0.7)\n",
    "ax3.axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='α = 0.05')\n",
    "ax3.set_title('P-values Médios (Chi-squared)', fontweight='bold')\n",
    "ax3.set_xlabel('Tipo de Drift')\n",
    "ax3.set_ylabel('P-value Médio')\n",
    "ax3.set_yscale('log')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, p_val in zip(bars4, comparison_df['avg_p_value']):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() * 1.5,\n",
    "            f'{p_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Subplot 4: Effectiveness comparison (heatmap style)\n",
    "methods = ['PSI', 'Chi-squared']\n",
    "effectiveness_data = np.array([\n",
    "    comparison_df['psi_detected'].values,\n",
    "    comparison_df['chi2_detected'].values\n",
    "])\n",
    "\n",
    "im = ax4.imshow(effectiveness_data, cmap='RdYlGn', aspect='auto')\n",
    "ax4.set_title('Mapa de Efetividade\\n(Features Detectadas)', fontweight='bold')\n",
    "ax4.set_xticks(range(len(drift_types_comp)))\n",
    "ax4.set_xticklabels(drift_types_comp, rotation=45)\n",
    "ax4.set_yticks(range(len(methods)))\n",
    "ax4.set_yticklabels(methods)\n",
    "\n",
    "# Adicionar valores no heatmap\n",
    "for i in range(len(methods)):\n",
    "    for j in range(len(drift_types_comp)):\n",
    "        text = ax4.text(j, i, int(effectiveness_data[i, j]),\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Resumo final\n",
    "print(f\"\\n🎯 RESUMO COMPARATIVO FINAL:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"📊 Tabela Comparativa:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "print(f\"\\n🔍 ANÁLISE COMPARATIVA:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    drift = row['drift_type']\n",
    "    psi_det = row['psi_detected']\n",
    "    chi2_det = row['chi2_detected']\n",
    "    affected = row['features_affected']\n",
    "    \n",
    "    print(f\"\\n🔸 {drift.upper()}:\")\n",
    "    print(f\"   Features alteradas: {affected}\")\n",
    "    print(f\"   PSI detectou: {psi_det} features\")\n",
    "    print(f\"   Chi² detectou: {chi2_det} features\")\n",
    "    \n",
    "    if psi_det == chi2_det:\n",
    "        comparison = \"🟰 Igual efetividade\"\n",
    "    elif psi_det > chi2_det:\n",
    "        comparison = f\"📈 PSI mais efetivo (+{psi_det-chi2_det})\"\n",
    "    else:\n",
    "        comparison = f\"📉 Chi² mais efetivo (+{chi2_det-psi_det})\"\n",
    "    \n",
    "    print(f\"   Resultado: {comparison}\")\n",
    "\n",
    "print(f\"\\n💡 CONCLUSÕES GERAIS:\")\n",
    "print(\"=\" * 25)\n",
    "print(\"✅ Ambas as métricas são COMPATÍVEIS com a função induce_drift\")\n",
    "print(\"📈 PSI: Melhor para mudanças graduais e monitoramento contínuo\")\n",
    "print(\"🔬 Chi²: Melhor para testes de hipóteses e validação estatística\")\n",
    "print(\"🎯 Recomendação: Usar ambas em conjunto para análise completa\")\n",
    "print(\"\\n🔧 Casos de Uso Recomendados:\")\n",
    "print(\"• PSI: Monitoramento em produção, alertas automáticos\")\n",
    "print(\"• Chi²: Validação científica, análise exploratória, relatórios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def induce_specific_drifts(test_df, feature_names):\n",
    "    \"\"\"Induce specific types of drift for testing different metrics.\"\"\"\n",
    "    \n",
    "    scenarios = {}\n",
    "    \n",
    "    # Scenario 1: Gradual mean shift (detectable by KL/JS divergence)\n",
    "    scenario_1 = test_df.copy()\n",
    "    numerical_features = [f for f in feature_names if f.startswith('feature_')]\n",
    "    target_feature = numerical_features[0]\n",
    "    \n",
    "    # Gradual shift that creates different distribution shapes\n",
    "    shift_values = np.linspace(0, 2, len(scenario_1))\n",
    "    scenario_1[target_feature] += shift_values * scenario_1[target_feature].std()\n",
    "    scenarios['gradual_mean_shift'] = scenario_1\n",
    "    \n",
    "    # Scenario 2: Distribution shape change (strong KL divergence signal)\n",
    "    scenario_2 = test_df.copy()\n",
    "    target_feature = numerical_features[1]\n",
    "    \n",
    "    # Transform from normal to exponential-like distribution\n",
    "    original_data = scenario_2[target_feature]\n",
    "    # Apply exponential transformation while preserving some original characteristics\n",
    "    transformed_data = np.random.exponential(scale=np.abs(original_data.mean()), size=len(original_data))\n",
    "    scenario_2[target_feature] = transformed_data\n",
    "    scenarios['distribution_shape_change'] = scenario_2\n",
    "    \n",
    "    # Scenario 3: Categorical frequency drift (detectable by Chi-square and categorical KL)\n",
    "    scenario_3 = test_df.copy()\n",
    "    \n",
    "    # Change category distribution significantly\n",
    "    new_categories = np.random.choice(['Type_A', 'Type_B', 'Type_C'], \n",
    "                                    size=len(scenario_3), \n",
    "                                    p=[0.1, 0.2, 0.7])  # Very different from original [0.5, 0.3, 0.2]\n",
    "    scenario_3['category_1'] = new_categories\n",
    "    scenarios['categorical_frequency_drift'] = scenario_3\n",
    "    \n",
    "    # Scenario 4: Multiple subtle drifts (low individual signals, but cumulative effect)\n",
    "    scenario_4 = test_df.copy()\n",
    "    \n",
    "    # Small shifts in multiple features\n",
    "    for i, feature in enumerate(numerical_features[:4]):\n",
    "        shift = 0.3 * scenario_4[feature].std() * (i + 1) / 4  # Increasing shifts\n",
    "        scenario_4[feature] += shift\n",
    "    \n",
    "    # Slight categorical change\n",
    "    mask = np.random.choice([True, False], size=len(scenario_4), p=[0.2, 0.8])\n",
    "    scenario_4.loc[mask, 'category_2'] = 'Type_A'\n",
    "    scenarios['multiple_subtle_drifts'] = scenario_4\n",
    "    \n",
    "    return scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5b515b",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main function to run the comprehensive comparison.\"\"\"\n",
    "    \n",
    "print(\"Advanced Drift Metrics Integration Example\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating synthetic datasets with various drift scenarios...\")\n",
    "reference_df, test_df, y_ref, y_test, feature_names = create_enhanced_drift_scenarios()\n",
    "\n",
    "# Create drift scenarios\n",
    "print(\"Inducing different types of drift...\")\n",
    "drifted_scenarios = induce_specific_drifts(test_df, feature_names)\n",
    "\n",
    "# Compare detection methods\n",
    "print(\"Comparing drift detection methods...\")\n",
    "comparison_results = compare_drift_detection_methods(\n",
    "    reference_df, drifted_scenarios, y_ref, y_test, feature_names\n",
    ")\n",
    "\n",
    "# Create summary comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY COMPARISON OF DETECTION METHODS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for scenario_name, results in comparison_results.items():\n",
    "    print(f\"\\nScenario: {scenario_name}\")\n",
    "    xadapt_count = len(results['xadapt_drifted'])\n",
    "    advanced_count = len(results['advanced_drifted'])\n",
    "    \n",
    "    print(f\"  Standard XAdapt-Drift detected: {xadapt_count} features\")\n",
    "    print(f\"  Advanced metrics detected: {advanced_count} features\")\n",
    "    \n",
    "    # Find agreement and disagreement\n",
    "    agreement = results['xadapt_drifted'] & results['advanced_drifted']\n",
    "    only_xadapt = results['xadapt_drifted'] - results['advanced_drifted']\n",
    "    only_advanced = results['advanced_drifted'] - results['xadapt_drifted']\n",
    "    \n",
    "    print(f\"  Agreement on: {list(agreement)}\")\n",
    "    if only_xadapt:\n",
    "        print(f\"  Only XAdapt-Drift detected: {list(only_xadapt)}\")\n",
    "    if only_advanced:\n",
    "        print(f\"  Only advanced metrics detected: {list(only_advanced)}\")\n",
    "    \n",
    "    print(f\"  Average KL divergence: {results['advanced_summary']['average_kl_divergence']:.4f}\")\n",
    "    print(f\"  Average JS divergence: {results['advanced_summary']['average_js_divergence']:.4f}\")\n",
    "\n",
    "# Visualize one example\n",
    "scenario_to_visualize = 'distribution_shape_change'\n",
    "if scenario_to_visualize in drifted_scenarios:\n",
    "    print(f\"\\nCreating visualization for {scenario_to_visualize} scenario...\")\n",
    "    numerical_features = [f for f in feature_names if f.startswith('feature_')]\n",
    "    target_feature = numerical_features[1]  # The one we modified\n",
    "    \n",
    "    fig = visualize_drift_metrics_comparison(\n",
    "        reference_df, drifted_scenarios[scenario_to_visualize], target_feature\n",
    "    )\n",
    "    \n",
    "    if fig:\n",
    "        plt.savefig(f'drift_metrics_comparison_{target_feature}.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Visualization saved as 'drift_metrics_comparison_{target_feature}.png'\")\n",
    "\n",
    "print(\"\\nAnalysis complete! The advanced metrics provide additional insights into the nature and magnitude of drift.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
